{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00000-863594dd-75cd-49b8-aa4e-33d35cd3fd87",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636997946520,
    "source_hash": "97081a08",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "try:\n",
    "    from country_region import *\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00001-479b1a1c-dfca-49f9-83e0-a3d4d89c2f15",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1636997946536,
    "source_hash": "7423d502",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "fix_all_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-b0783bee-3331-4ecb-98f2-55cdca168ac3",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00003-781ad054-f142-4a34-9a24-9b9a9d2f0583",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 16339,
    "execution_start": 1636997949420,
    "source_hash": "89ec949a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta = pd.read_csv('4000_PCS_human_origins/v44.3_HO_public.anno', sep='\\t')\n",
    "pcs = pd.read_csv('4000_PCS_human_origins/pcs.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Version ID</th>\n",
       "      <th>Master ID</th>\n",
       "      <th>Publication (or OK to use in a paper)</th>\n",
       "      <th>Representative contact</th>\n",
       "      <th>Date mean in BP [OxCal mu for a direct radiocarbon date, and average of range for a contextual date]</th>\n",
       "      <th>Full Date: One of two formats. (Format 1) 95.4% CI calibrated radiocarbon age (Conventional Radiocarbon Age BP, Lab number) e.g. 2624-2350 calBCE (3990Â±40 BP, Ua-35016). (Format 2) Archaeological context range, e.g. 2500-1700 BCE</th>\n",
       "      <th>Group Label</th>\n",
       "      <th>Locality</th>\n",
       "      <th>Country</th>\n",
       "      <th>Lat.</th>\n",
       "      <th>Long.</th>\n",
       "      <th>Data source</th>\n",
       "      <th>Coverage on autosomal targets</th>\n",
       "      <th>SNPs hit on autosomal targets</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Library type (minus=no.damage.correction, half=damage.retained.at.last.position, plus=damage.fully.corrected, ds=double.stranded.library.preparation, ss=single.stranded.library.preparation)</th>\n",
       "      <th>ASSESSMENT (Xcontam interval is listed if lower bound is &gt;0.005, \"QUESTIONABLE\" if lower bound is 0.01-0.02, \"QUESTIONABLE_CRITICAL\" or \"FAIL\" if lower bound is &gt;0.02) (mtcontam confidence interval is listed if coverage &gt;2 and upper bound is &lt;0.98: 0.9-0.95 is \"QUESTIONABLE\"; &lt;0.9 is \"QUESTIONABLE_CRITICAL\", questionable status gets overriden by ANGSD with PASS if upper bound of contamination is &lt;0.01 and QUESTIONABLE if upper bound is 0.01-0.05) (damage for ds.half is \"QUESTIONABLE_CRITICAL/FAIL\" if &lt;0.01, \"QUESTIONABLE\" for 0.01-0.03, and recorded but passed if 0.03-0.05; libraries with fully-treated last base are \"QUESTIONABLE_CRITICAL\" or \"FAIL\" if &lt;0.03, \"QUESTIONABLE\" if 0.03-0.06, and recorded but passed if 0.06-0.1) (sexratio is QUESTIONABLE if [0.03,0.10] or [0.30,0.35); QUESTIONABLE_CRITICAL/FAIL if (0.10,0.30))</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1798</td>\n",
       "      <td>MAL-005</td>\n",
       "      <td>MAL-005</td>\n",
       "      <td>SkoglundCell2017</td>\n",
       "      <td>Garrett Hellenthal / Saioa Lopez / Mark Thomas...</td>\n",
       "      <td>0</td>\n",
       "      <td>..</td>\n",
       "      <td>Malawi_Yao</td>\n",
       "      <td>Dedza // Yao</td>\n",
       "      <td>Malawi</td>\n",
       "      <td>-14.166667</td>\n",
       "      <td>34.33333</td>\n",
       "      <td>Fall2015</td>\n",
       "      <td>..</td>\n",
       "      <td>585645</td>\n",
       "      <td>M</td>\n",
       "      <td>..</td>\n",
       "      <td>PASS (genotyping)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1799</td>\n",
       "      <td>MAL-009</td>\n",
       "      <td>MAL-009</td>\n",
       "      <td>SkoglundCell2017</td>\n",
       "      <td>Garrett Hellenthal / Saioa Lopez / Mark Thomas...</td>\n",
       "      <td>0</td>\n",
       "      <td>..</td>\n",
       "      <td>Malawi_Yao</td>\n",
       "      <td>Machinga // Yao</td>\n",
       "      <td>Malawi</td>\n",
       "      <td>-14.862605</td>\n",
       "      <td>35.574122</td>\n",
       "      <td>Fall2015</td>\n",
       "      <td>..</td>\n",
       "      <td>582189</td>\n",
       "      <td>M</td>\n",
       "      <td>..</td>\n",
       "      <td>PASS (genotyping)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1800</td>\n",
       "      <td>MAL-011</td>\n",
       "      <td>MAL-011</td>\n",
       "      <td>SkoglundCell2017</td>\n",
       "      <td>Garrett Hellenthal / Saioa Lopez / Mark Thomas...</td>\n",
       "      <td>0</td>\n",
       "      <td>..</td>\n",
       "      <td>Malawi_Chewa</td>\n",
       "      <td>Mchinga // Chichewa</td>\n",
       "      <td>Malawi</td>\n",
       "      <td>-14.862605</td>\n",
       "      <td>35.574122</td>\n",
       "      <td>Fall2015</td>\n",
       "      <td>..</td>\n",
       "      <td>579844</td>\n",
       "      <td>M</td>\n",
       "      <td>..</td>\n",
       "      <td>PASS (genotyping)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1801</td>\n",
       "      <td>MAL-012</td>\n",
       "      <td>MAL-012</td>\n",
       "      <td>SkoglundCell2017</td>\n",
       "      <td>Garrett Hellenthal / Saioa Lopez / Mark Thomas...</td>\n",
       "      <td>0</td>\n",
       "      <td>..</td>\n",
       "      <td>Malawi_Chewa</td>\n",
       "      <td>Salima // Chichewa</td>\n",
       "      <td>Malawi</td>\n",
       "      <td>-13.75</td>\n",
       "      <td>34.5</td>\n",
       "      <td>Fall2015</td>\n",
       "      <td>..</td>\n",
       "      <td>585204</td>\n",
       "      <td>M</td>\n",
       "      <td>..</td>\n",
       "      <td>PASS (genotyping)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1802</td>\n",
       "      <td>MAL-014</td>\n",
       "      <td>MAL-014</td>\n",
       "      <td>SkoglundCell2017</td>\n",
       "      <td>Garrett Hellenthal / Saioa Lopez / Mark Thomas...</td>\n",
       "      <td>0</td>\n",
       "      <td>..</td>\n",
       "      <td>Malawi_Chewa</td>\n",
       "      <td>Nambuma // Chichewa</td>\n",
       "      <td>Malawi</td>\n",
       "      <td>-13.703473</td>\n",
       "      <td>33.597743</td>\n",
       "      <td>Fall2015</td>\n",
       "      <td>..</td>\n",
       "      <td>584410</td>\n",
       "      <td>M</td>\n",
       "      <td>..</td>\n",
       "      <td>PASS (genotyping)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13192</th>\n",
       "      <td>31953</td>\n",
       "      <td>VK94.SG</td>\n",
       "      <td>VK94</td>\n",
       "      <td>MargaryanWillerslevNature2020</td>\n",
       "      <td>Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske</td>\n",
       "      <td>1000</td>\n",
       "      <td>800-1100 CE</td>\n",
       "      <td>Denmark_Viking.SG</td>\n",
       "      <td>Sealand, Gl. Lejre</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>55.62</td>\n",
       "      <td>11.97</td>\n",
       "      <td>Shotgun</td>\n",
       "      <td>0.14</td>\n",
       "      <td>63365</td>\n",
       "      <td>F</td>\n",
       "      <td>ds.minus</td>\n",
       "      <td>PASS (literature)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13193</th>\n",
       "      <td>31954</td>\n",
       "      <td>VK95.SG</td>\n",
       "      <td>VK95</td>\n",
       "      <td>MargaryanWillerslevNature2020</td>\n",
       "      <td>Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske</td>\n",
       "      <td>850</td>\n",
       "      <td>900-1300 CE</td>\n",
       "      <td>Iceland_Viking.SG</td>\n",
       "      <td>Hofstadir</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>65.61</td>\n",
       "      <td>-17.16</td>\n",
       "      <td>Shotgun</td>\n",
       "      <td>1.32</td>\n",
       "      <td>422417</td>\n",
       "      <td>M</td>\n",
       "      <td>ds.minus</td>\n",
       "      <td>PASS (literature)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13194</th>\n",
       "      <td>31955</td>\n",
       "      <td>VK98.SG</td>\n",
       "      <td>VK98</td>\n",
       "      <td>MargaryanWillerslevNature2020</td>\n",
       "      <td>Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske</td>\n",
       "      <td>850</td>\n",
       "      <td>900-1300 CE</td>\n",
       "      <td>Iceland_Viking.SG</td>\n",
       "      <td>Hofstadir</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>65.61</td>\n",
       "      <td>-17.16</td>\n",
       "      <td>Shotgun</td>\n",
       "      <td>2.49</td>\n",
       "      <td>524046</td>\n",
       "      <td>M</td>\n",
       "      <td>ds.minus</td>\n",
       "      <td>PASS (literature)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13195</th>\n",
       "      <td>31956</td>\n",
       "      <td>VK99.SG</td>\n",
       "      <td>VK99</td>\n",
       "      <td>MargaryanWillerslevNature2020</td>\n",
       "      <td>Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske</td>\n",
       "      <td>850</td>\n",
       "      <td>900-1300 CE</td>\n",
       "      <td>Iceland_Viking.SG</td>\n",
       "      <td>Hofstadir</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>65.61</td>\n",
       "      <td>-17.16</td>\n",
       "      <td>Shotgun</td>\n",
       "      <td>0.74</td>\n",
       "      <td>320753</td>\n",
       "      <td>F</td>\n",
       "      <td>ds.minus</td>\n",
       "      <td>PASS (literature)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13196</th>\n",
       "      <td>31957</td>\n",
       "      <td>VK9.SG</td>\n",
       "      <td>VK9</td>\n",
       "      <td>MargaryanWillerslevNature2020</td>\n",
       "      <td>Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske</td>\n",
       "      <td>950</td>\n",
       "      <td>900-1100 CE</td>\n",
       "      <td>Greenland_EarlyNorse_o2.SG</td>\n",
       "      <td>Eastern Settlement, 64</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>60.99</td>\n",
       "      <td>-45.42</td>\n",
       "      <td>Shotgun</td>\n",
       "      <td>0.1</td>\n",
       "      <td>34799</td>\n",
       "      <td>F</td>\n",
       "      <td>ds.minus</td>\n",
       "      <td>PASS (literature)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13197 rows Ã 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Index Version ID Master ID Publication (or OK to use in a paper)  \\\n",
       "0       1798    MAL-005   MAL-005                      SkoglundCell2017   \n",
       "1       1799    MAL-009   MAL-009                      SkoglundCell2017   \n",
       "2       1800    MAL-011   MAL-011                      SkoglundCell2017   \n",
       "3       1801    MAL-012   MAL-012                      SkoglundCell2017   \n",
       "4       1802    MAL-014   MAL-014                      SkoglundCell2017   \n",
       "...      ...        ...       ...                                   ...   \n",
       "13192  31953    VK94.SG      VK94         MargaryanWillerslevNature2020   \n",
       "13193  31954    VK95.SG      VK95         MargaryanWillerslevNature2020   \n",
       "13194  31955    VK98.SG      VK98         MargaryanWillerslevNature2020   \n",
       "13195  31956    VK99.SG      VK99         MargaryanWillerslevNature2020   \n",
       "13196  31957     VK9.SG       VK9         MargaryanWillerslevNature2020   \n",
       "\n",
       "                                  Representative contact  \\\n",
       "0      Garrett Hellenthal / Saioa Lopez / Mark Thomas...   \n",
       "1      Garrett Hellenthal / Saioa Lopez / Mark Thomas...   \n",
       "2      Garrett Hellenthal / Saioa Lopez / Mark Thomas...   \n",
       "3      Garrett Hellenthal / Saioa Lopez / Mark Thomas...   \n",
       "4      Garrett Hellenthal / Saioa Lopez / Mark Thomas...   \n",
       "...                                                  ...   \n",
       "13192   Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske   \n",
       "13193   Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske   \n",
       "13194   Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske   \n",
       "13195   Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske   \n",
       "13196   Nielsen, Rasmus; Werge, Thomas; Willerslev, Eske   \n",
       "\n",
       "       Date mean in BP [OxCal mu for a direct radiocarbon date, and average of range for a contextual date]  \\\n",
       "0                                                      0                                                      \n",
       "1                                                      0                                                      \n",
       "2                                                      0                                                      \n",
       "3                                                      0                                                      \n",
       "4                                                      0                                                      \n",
       "...                                                  ...                                                      \n",
       "13192                                               1000                                                      \n",
       "13193                                                850                                                      \n",
       "13194                                                850                                                      \n",
       "13195                                                850                                                      \n",
       "13196                                                950                                                      \n",
       "\n",
       "      Full Date: One of two formats. (Format 1) 95.4% CI calibrated radiocarbon age (Conventional Radiocarbon Age BP, Lab number) e.g. 2624-2350 calBCE (3990Â±40 BP, Ua-35016). (Format 2) Archaeological context range, e.g. 2500-1700 BCE  \\\n",
       "0                                                     ..                                                                                                                                                                                      \n",
       "1                                                     ..                                                                                                                                                                                      \n",
       "2                                                     ..                                                                                                                                                                                      \n",
       "3                                                     ..                                                                                                                                                                                      \n",
       "4                                                     ..                                                                                                                                                                                      \n",
       "...                                                  ...                                                                                                                                                                                      \n",
       "13192                                        800-1100 CE                                                                                                                                                                                      \n",
       "13193                                        900-1300 CE                                                                                                                                                                                      \n",
       "13194                                        900-1300 CE                                                                                                                                                                                      \n",
       "13195                                        900-1300 CE                                                                                                                                                                                      \n",
       "13196                                        900-1100 CE                                                                                                                                                                                      \n",
       "\n",
       "                      Group Label                Locality    Country  \\\n",
       "0                      Malawi_Yao            Dedza // Yao     Malawi   \n",
       "1                      Malawi_Yao         Machinga // Yao     Malawi   \n",
       "2                    Malawi_Chewa     Mchinga // Chichewa     Malawi   \n",
       "3                    Malawi_Chewa      Salima // Chichewa     Malawi   \n",
       "4                    Malawi_Chewa     Nambuma // Chichewa     Malawi   \n",
       "...                           ...                     ...        ...   \n",
       "13192           Denmark_Viking.SG      Sealand, Gl. Lejre    Denmark   \n",
       "13193           Iceland_Viking.SG               Hofstadir    Iceland   \n",
       "13194           Iceland_Viking.SG               Hofstadir    Iceland   \n",
       "13195           Iceland_Viking.SG               Hofstadir    Iceland   \n",
       "13196  Greenland_EarlyNorse_o2.SG  Eastern Settlement, 64  Greenland   \n",
       "\n",
       "             Lat.      Long. Data source Coverage on autosomal targets  \\\n",
       "0      -14.166667   34.33333    Fall2015                            ..   \n",
       "1      -14.862605  35.574122    Fall2015                            ..   \n",
       "2      -14.862605  35.574122    Fall2015                            ..   \n",
       "3          -13.75       34.5    Fall2015                            ..   \n",
       "4      -13.703473  33.597743    Fall2015                            ..   \n",
       "...           ...        ...         ...                           ...   \n",
       "13192       55.62      11.97     Shotgun                          0.14   \n",
       "13193       65.61     -17.16     Shotgun                          1.32   \n",
       "13194       65.61     -17.16     Shotgun                          2.49   \n",
       "13195       65.61     -17.16     Shotgun                          0.74   \n",
       "13196       60.99     -45.42     Shotgun                           0.1   \n",
       "\n",
       "       SNPs hit on autosomal targets Sex  \\\n",
       "0                             585645   M   \n",
       "1                             582189   M   \n",
       "2                             579844   M   \n",
       "3                             585204   M   \n",
       "4                             584410   M   \n",
       "...                              ...  ..   \n",
       "13192                          63365   F   \n",
       "13193                         422417   M   \n",
       "13194                         524046   M   \n",
       "13195                         320753   F   \n",
       "13196                          34799   F   \n",
       "\n",
       "      Library type (minus=no.damage.correction, half=damage.retained.at.last.position, plus=damage.fully.corrected, ds=double.stranded.library.preparation, ss=single.stranded.library.preparation)  \\\n",
       "0                                                     ..                                                                                                                                              \n",
       "1                                                     ..                                                                                                                                              \n",
       "2                                                     ..                                                                                                                                              \n",
       "3                                                     ..                                                                                                                                              \n",
       "4                                                     ..                                                                                                                                              \n",
       "...                                                  ...                                                                                                                                              \n",
       "13192                                           ds.minus                                                                                                                                              \n",
       "13193                                           ds.minus                                                                                                                                              \n",
       "13194                                           ds.minus                                                                                                                                              \n",
       "13195                                           ds.minus                                                                                                                                              \n",
       "13196                                           ds.minus                                                                                                                                              \n",
       "\n",
       "      ASSESSMENT (Xcontam interval is listed if lower bound is >0.005, \"QUESTIONABLE\" if lower bound is 0.01-0.02, \"QUESTIONABLE_CRITICAL\" or \"FAIL\" if lower bound is >0.02) (mtcontam confidence interval is listed if coverage >2 and upper bound is <0.98: 0.9-0.95 is \"QUESTIONABLE\"; <0.9 is \"QUESTIONABLE_CRITICAL\", questionable status gets overriden by ANGSD with PASS if upper bound of contamination is <0.01 and QUESTIONABLE if upper bound is 0.01-0.05) (damage for ds.half is \"QUESTIONABLE_CRITICAL/FAIL\" if <0.01, \"QUESTIONABLE\" for 0.01-0.03, and recorded but passed if 0.03-0.05; libraries with fully-treated last base are \"QUESTIONABLE_CRITICAL\" or \"FAIL\" if <0.03, \"QUESTIONABLE\" if 0.03-0.06, and recorded but passed if 0.06-0.1) (sexratio is QUESTIONABLE if [0.03,0.10] or [0.30,0.35); QUESTIONABLE_CRITICAL/FAIL if (0.10,0.30))  \n",
       "0                                      PASS (genotyping)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1                                      PASS (genotyping)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "2                                      PASS (genotyping)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "3                                      PASS (genotyping)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "4                                      PASS (genotyping)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "...                                                  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "13192                                  PASS (literature)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "13193                                  PASS (literature)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "13194                                  PASS (literature)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "13195                                  PASS (literature)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "13196                                  PASS (literature)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "\n",
       "[13197 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-44279d32-2e67-4c10-b483-fa7e87f3ed1f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Prepare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00005-80aafdcb-3a16-40fa-9012-2242af563436",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1636998017924,
    "source_hash": "6ba2f784",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "\n",
    "def prepare_data(pcs: pd.DataFrame, meta: pd.DataFrame, num_of_pcs: Union[None, int]=None, target: str='region') -> Tuple[torch.Tensor, torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    pcs: The principal components of the data\n",
    "    meta: The meta data of the datam, as given by https://reichdata.hms.harvard.edu/pub/datasets/amh_repo/curated_releases/index_v44.3.html\n",
    "    num_of_pcs: The number of principal components to include from the pcs dataframe. If None, all pcs are included.\n",
    "\n",
    "    Output:\n",
    "        x: is the features in a torch Tensor\n",
    "        y: is the targets in a torch Tensor\n",
    "        idx_to_country: a dictionary that maps the index in the target vector, to the representative country\n",
    "    \"\"\"\n",
    "    \n",
    "    filter_meta = meta[['Version ID', 'Country']]\n",
    "\n",
    "    merged = pd.merge(filter_meta, pcs, how='left', left_on=['Version ID'], right_on=['IID'])\n",
    "    merged = merged[merged['Country'] != '..'].reset_index(drop=True) # first country is '..', so we remove that\n",
    "    \n",
    "    \n",
    "    if target.lower() == 'country':\n",
    "        countries = merged['Country'].unique()\n",
    "        idx_to_country = dict(zip(countries, np.arange(len(countries))))\n",
    "        merged['Country'] = merged.apply(lambda row: idx_to_country[row['Country']], axis=1)\n",
    "        # one_hot_countries = pd.get_dummies(merged['Country'])  \n",
    "    \n",
    "    elif target.lower() == 'region':\n",
    "        merged['Region'] = merged['Country'].map(country_region)\n",
    "        regions = merged['Region'].unique()\n",
    "        idx_to_region = dict(zip(regions, np.arange(len(regions))))\n",
    "        merged['Region'] = merged.apply(lambda row: idx_to_country[row['Region']], axis=1)\n",
    "    \n",
    "    \n",
    "    # y = torch.from_numpy(one_hot_countries.values)\n",
    "    y = torch.from_numpy(merged['Country'].values)\n",
    "    x_all = merged.iloc[:, 4:].values\n",
    "\n",
    "    if num_of_pcs is None:\n",
    "        x = x_all\n",
    "    else:\n",
    "        x = x_all[:, :num_of_pcs]\n",
    "    \n",
    "    x = torch.from_numpy(x)\n",
    "    \n",
    "    # countries = one_hot_countries.columns\n",
    "    # idx_to_country = dict(zip(np.arange(len(countries)), countries))\n",
    "\n",
    "    return x.float(), y.long(), idx_to_country\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00006-07e127b4-8ef1-4392-8127-34ab7c7b7699",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1636998019710,
    "source_hash": "cff6453",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(x: torch.Tensor, y: torch.Tensor, batch_size: int = 64, train_shuffle: bool = False, save: bool = False):\n",
    "    \"\"\"\n",
    "    x: Features which were created in the prepare_data function\n",
    "    y: Targets which were created in the prepare_data function\n",
    "    batch_size: Chooses the batch size of the dataloader\n",
    "    save: If the dataloaders should be saved\n",
    "\n",
    "    Output: Two dataloaders, one for training and another for validation\n",
    "    \"\"\"\n",
    "    \n",
    "    # First the full dataset is created from the tensors\n",
    "    dataset = TensorDataset(x, y)\n",
    "\n",
    "    # Then using random_split a test and val set is created\n",
    "    train_size = int(0.8*len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_set, val_set = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    # Then the dataloaders can initialized\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=train_shuffle)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    if save:\n",
    "        torch.save(train_loader, 'processed_data/train_loader.pth')\n",
    "        torch.save(val_loader, 'processed_data/val_loader.pth')\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00007-ae955bb7-f8ac-44b5-ada7-eaa7091e323a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4139,
    "execution_start": 1636998145418,
    "source_hash": "35ca03ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "x, y, idx_to_country = prepare_data(pcs, meta, num_of_pcs=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00008-81a521f1-474a-42c6-a871-be2592afbe4f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1636998234699,
    "source_hash": "b675993a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader, val_loader = create_dataloaders(x, y, batch_size=64, train_shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-5b36e1fa-75b2-4af3-b9ed-7f4e59b2435a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00010-2362a30c-c69e-4839-816d-887940a47d17",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1636998288897,
    "source_hash": "9d9abd78",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple ffnn\n",
    "class ffnn(pl.LightningModule):\n",
    "    def __init__(self, input_size: int=100, layer_1: int=2000, layer_2: int=1500, layer_3: int=1000, layer_4: int=500,\n",
    "                 dropout: float=0.25,\n",
    "                 lr: float=1.5e-4, weight_decay: float=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=input_size,\n",
    "            out_features=layer_1\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=layer_1,\n",
    "            out_features=layer_2\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=layer_2,\n",
    "            out_features=layer_3\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Linear(\n",
    "            in_features=layer_3,\n",
    "            out_features=layer_4\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(\n",
    "            in_features=layer_4,\n",
    "            out_features=14  # Number of regions  # 143 countries\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(layer_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(layer_2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(layer_3)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(layer_4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm1(z)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        z = self.fc2(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm2(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        z = self.fc3(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm3(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        z = self.fc4(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm4(z)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        out = self.fc_out(z)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        self.log('train_acc', acc, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "        self.log('val_loss', loss, on_epoch=True)\n",
    "        self.log('val_acc', acc, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-cee7b4e8-d2c3-4e5e-8133-363824d505c1",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Training using PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00012-ad8d9fb5-ef7f-42f2-b972-d272c07a5dd6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 202107,
    "execution_start": 1636989422422,
    "source_hash": "7770b282",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | fc1        | Linear      | 8.0 M \n",
      "1 | fc2        | Linear      | 3.0 M \n",
      "2 | fc3        | Linear      | 1.5 M \n",
      "3 | fc4        | Linear      | 500 K \n",
      "4 | fc_out     | Linear      | 71.6 K\n",
      "5 | dropout    | Dropout     | 0     \n",
      "6 | batchnorm1 | BatchNorm1d | 4.0 K \n",
      "7 | batchnorm2 | BatchNorm1d | 3.0 K \n",
      "8 | batchnorm3 | BatchNorm1d | 2.0 K \n",
      "9 | batchnorm4 | BatchNorm1d | 1.0 K \n",
      "-------------------------------------------\n",
      "13.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.1 M    Total params\n",
      "52.347    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74295dc3d233494fa43b2ba35d7ec9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu = 1 if torch.cuda.is_available() else 0\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # logger=True,\n",
    "    # log_every_n_steps=5,\n",
    "    gpus=gpu, \n",
    "    max_epochs=15\n",
    ")\n",
    "\n",
    "hyperparameters = dict(n_layers=4)\n",
    "    \n",
    "trainer.logger.log_hyperparams(hyperparameters)\n",
    "\n",
    "model = ffnn(input_size=4000)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00013-c76cb002-ee14-46d8-88d8-e3badbfd4abd",
    "deepnote_cell_type": "code",
    "deepnote_output_heights": [
     155.60000610351562
    ],
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 46,
    "execution_start": 1636989629308,
    "source_hash": "bd081cc1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(0.),\n",
       " 'train_loss_step': tensor(0.),\n",
       " 'train_acc': tensor(0.),\n",
       " 'train_acc_step': tensor(0.),\n",
       " 'val_loss': tensor(0.),\n",
       " 'val_acc': tensor(0.),\n",
       " 'train_loss_epoch': tensor(0.),\n",
       " 'train_acc_epoch': tensor(0.)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I found:\n",
    "\n",
    "After trial and error:\n",
    "- 4 layers is optimal compared to 3 and 5\n",
    "- At around 8 epochs the validation error starts to increase\n",
    "- Batchnorm gives a higher accuracy\n",
    "\n",
    "## Optuna\n",
    "\n",
    "Trial with only:\n",
    "- Batch size\n",
    "- Input size\n",
    "- Dropout\n",
    "\n",
    "Best trial:\n",
    "\n",
    "\t Value: 0.7513267397880554\n",
    "\t Params:\n",
    "\t \t batch_size: 45\n",
    "\t \t input_size: 296\n",
    "\t \t dropout: 0.1760540639257987\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00014-28f5d55f-fc02-4674-a44d-3b8a98afed1e",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters that is optimized\n",
    "    batch_size = trial.suggest_int('batch_size', 12, 64)\n",
    "    \n",
    "    # Model inputs\n",
    "    input_size = trial.suggest_int('input_size', 100, 4000)\n",
    "    layer_1 = trial.suggest_int('layer_1', 500, 2000)\n",
    "    layer_2 = trial.suggest_int('layer_2', 500, 2000)\n",
    "    layer_3 = trial.suggest_int('layer_3', 500, 2000)\n",
    "    layer_4 = trial.suggest_int('layer_4', 500, 2000)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    \n",
    "    # Optimizer values\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-7, 10-2)\n",
    "    \n",
    "    x_trial = x[:, :input_size]\n",
    "    train_loader, val_loader = create_dataloaders(x_trial, y, batch_size=batch_size, train_shuffle=True)\n",
    "    \n",
    "    model = ffnn(input_size=input_size, layer_1=layer_1, layer_2=layer_2, layer_3=layer_3, layer_4=layer_4,\n",
    "                 dropout=dropout, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    gpu = 1 if torch.cuda.is_available() else 0\n",
    "    trainer = pl.Trainer(\n",
    "        logger=True,\n",
    "        max_epochs=10,\n",
    "        # log_every_n_steps=5,\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor='val/acc')],\n",
    "        gpus=gpu,\n",
    "    )\n",
    "    \n",
    "    hyperparameters = dict(batch_size=batch_size,\n",
    "                           input_size=input_size,\n",
    "                           layer_1=layer_1,\n",
    "                           layer_2=layer_2,\n",
    "                           layer_3=layer_3,\n",
    "                           layer_4=layer_4,\n",
    "                           dropout=dropout, \n",
    "                           lr=lr,\n",
    "                           weight_decay=weight_decay)\n",
    "    \n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    return trainer.callback_metrics['val/acc'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00015-632fa7f8-4755-4bbb-9d1f-e2ec0df5d036",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-22 21:32:33,047]\u001b[0m A new study created in memory with name: no-name-3722b8e2-6d0e-44bf-9d64-0d623516c11c\u001b[0m\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal hyperparameters using Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | fc1        | Linear      | 2.2 M \n",
      "1 | fc2        | Linear      | 1.1 M \n",
      "2 | fc3        | Linear      | 905 K \n",
      "3 | fc4        | Linear      | 816 K \n",
      "4 | fc_out     | Linear      | 153 K \n",
      "5 | dropout    | Dropout     | 0     \n",
      "6 | batchnorm1 | BatchNorm1d | 1.9 K \n",
      "7 | batchnorm2 | BatchNorm1d | 2.4 K \n",
      "8 | batchnorm3 | BatchNorm1d | 1.5 K \n",
      "9 | batchnorm4 | BatchNorm1d | 2.1 K \n",
      "-------------------------------------------\n",
      "5.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.2 M     Total params\n",
      "20.904    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd72f926f2a4137a440ad94b0111ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\trial\\_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-22 21:34:29,979]\u001b[0m Trial 0 finished with value: 0.0 and parameters: {'batch_size': 15, 'input_size': 2394, 'layer_1': 933, 'layer_2': 1187, 'layer_3': 762, 'layer_4': 1070, 'dropout': 0.2534656143000949, 'lr': 0.0011215870575237517, 'weight_decay': 2.7167258723950174}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | fc1        | Linear      | 3.6 M \n",
      "1 | fc2        | Linear      | 697 K \n",
      "2 | fc3        | Linear      | 934 K \n",
      "3 | fc4        | Linear      | 2.9 M \n",
      "4 | fc_out     | Linear      | 274 K \n",
      "5 | dropout    | Dropout     | 0     \n",
      "6 | batchnorm1 | BatchNorm1d | 2.3 K \n",
      "7 | batchnorm2 | BatchNorm1d | 1.2 K \n",
      "8 | batchnorm3 | BatchNorm1d | 3.0 K \n",
      "9 | batchnorm4 | BatchNorm1d | 3.8 K \n",
      "-------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.985    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f77e8b17d5d40fa9ea6b352804f6b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\trial\\_trial.py:590: UserWarning: The reported value is ignored because this `step` 0 is already reported.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-11-22 21:36:41,287]\u001b[0m Trial 1 finished with value: 0.0 and parameters: {'batch_size': 15, 'input_size': 3205, 'layer_1': 1138, 'layer_2': 612, 'layer_3': 1524, 'layer_4': 1922, 'dropout': 0.29993135095483475, 'lr': 0.003943173123914619, 'weight_decay': 1.1628513211134908}. Best is trial 0 with value: 0.0.\u001b[0m\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | fc1        | Linear      | 1.5 M \n",
      "1 | fc2        | Linear      | 2.6 M \n",
      "2 | fc3        | Linear      | 1.0 M \n",
      "3 | fc4        | Linear      | 936 K \n",
      "4 | fc_out     | Linear      | 256 K \n",
      "5 | dropout    | Dropout     | 0     \n",
      "6 | batchnorm1 | BatchNorm1d | 2.7 K \n",
      "7 | batchnorm2 | BatchNorm1d | 3.8 K \n",
      "8 | batchnorm3 | BatchNorm1d | 1.0 K \n",
      "9 | batchnorm4 | BatchNorm1d | 3.6 K \n",
      "-------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.993    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebc83b80fe24d7489c72847c2006045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2021-11-22 21:36:41,927]\u001b[0m Trial 2 failed because of the following error: RuntimeError('CUDA error: an illegal memory access was encountered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\spetr\\AppData\\Local\\Temp/ipykernel_17344/1926304977.py\", line 48, in objective\n",
      "    trainer.fit(model, train_loader, val_loader)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 735, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 682, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 770, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1193, in _run\n",
      "    self._dispatch()\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1272, in _dispatch\n",
      "    self.training_type_plugin.start_training(self)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\", line 202, in start_training\n",
      "    self._results = trainer.run_stage()\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1282, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1312, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py\", line 234, in advance\n",
      "    self.epoch_loop.run(data_fetcher)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py\", line 195, in advance\n",
      "    batch_output = self.batch_loop.run(batch, batch_idx)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py\", line 88, in advance\n",
      "    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\", line 145, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\", line 215, in advance\n",
      "    result = self._run_optimization(\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\", line 277, in _run_optimization\n",
      "    self.trainer.fit_loop.epoch_loop.batch_loop._update_running_loss(result.loss)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py\", line 127, in _update_running_loss\n",
      "    self.accumulated_loss.append(current_loss)\n",
      "  File \"d:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py\", line 78, in append\n",
      "    x = x.to(self.memory)\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17344/294087960.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maximize'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpruner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best trial:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    398\u001b[0m             )\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17344/1926304977.py\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_hyperparams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val/acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[0;32m    733\u001b[0m             )\n\u001b[0;32m    734\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 735\u001b[1;33m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[0;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         )\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    680\u001b[0m         \"\"\"\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m         \u001b[1;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    768\u001b[0m         \u001b[1;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 770\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[1;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1270\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"pl.Trainer\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;31m# double dispatch to initiate the training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"pl.Trainer\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1281\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_training_epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_training_batch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[0mbatch_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0moptimizers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_active_optimizers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_frequencies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\base.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# type: ignore[override]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         result = self._run_optimization(\n\u001b[0m\u001b[0;32m    216\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py\u001b[0m in \u001b[0;36m_run_optimization\u001b[1;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_loop\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_running_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[1;31m# untoggle model params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py\u001b[0m in \u001b[0;36m_update_running_loss\u001b[1;34m(self, current_loss)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[1;31m# track total loss for logging (avoid mem leaks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulated_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0maccumulated_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccumulated_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\02456-deep-learning\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m# ensure same device and type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# store without grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "print('Finding optimal hyperparameters using Optuna...')\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5)    \n",
    "    \n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=50)\n",
    "    \n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f'\\t Value: {trial.value}')\n",
    "print(f'\\t Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'\\t \\t {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5e594743-000f-4ba5-a45b-f9a10d5b726f",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
