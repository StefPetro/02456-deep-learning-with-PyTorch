{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbce21f-fa2b-473b-b36b-8a7c4b3a7868",
   "metadata": {},
   "source": [
    "## Imports, seed and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49eac61-72a4-451a-9b5a-d055b629b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "try:\n",
    "    from country_region import *\n",
    "    from data_processing import *\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac68cae-6d79-4afe-bf6f-3fff25a7ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "fix_all_seeds(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecadae-c823-471a-a661-ea9d5af5a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('Project/4000_PCS_human_origins/v44.3_HO_public.anno', sep='\\t')\n",
    "pcs = pd.read_csv('Project/4000_PCS_human_origins/pcs.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b06420-fc95-4ed5-afa5-6001f5be411e",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c5df3-57f4-43a7-a461-5fe5317d240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple ffnn\n",
    "class ffnn(pl.LightningModule):\n",
    "    def __init__(self, input_size: int=100, layer_1: int=2000, layer_2: int=1500, layer_3: int=1000, layer_4: int=500,\n",
    "                 dropout: float=0.25,\n",
    "                 lr: float=1.5e-4, weight_decay: float=1e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=input_size,\n",
    "            out_features=layer_1\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=layer_1,\n",
    "            out_features=layer_2\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=layer_2,\n",
    "            out_features=layer_3\n",
    "        )\n",
    "        \n",
    "        self.fc4 = nn.Linear(\n",
    "            in_features=layer_3,\n",
    "            out_features=layer_4\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(\n",
    "            in_features=layer_4,\n",
    "            out_features=11  # Number of regions (in the dataset = 12, originally 14)  # 143 countries\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(layer_1)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(layer_2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(layer_3)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(layer_4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.fc1(x)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm1(z)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        z = self.fc2(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm2(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        z = self.fc3(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm3(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        z = self.fc4(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.batchnorm4(z)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        out = self.fc_out(z)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        return optimizer\n",
    "    \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "        self.log('train/loss', loss, on_epoch=True)\n",
    "        self.log('train/acc', acc, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "        self.log('val/loss', loss, on_epoch=True)\n",
    "        self.log('val/acc', acc, on_epoch=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bbd7b-d2dd-462e-861c-7345ca4dd5eb",
   "metadata": {},
   "source": [
    "## Define objective function and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b1caa-df39-4c87-95e7-69a586157698",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 100\n",
    "x, y, idx_to_country = prepare_data(pcs, meta, num_of_pcs=INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70314d-b7a6-4b80-8a21-94b42aa598f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters that is optimized\n",
    "    batch_size = trial.suggest_int('batch_size', 12, 64)\n",
    "    \n",
    "    # Model inputs\n",
    "    layer_1 = trial.suggest_int('layer_1', 100, 2000)\n",
    "    layer_2 = trial.suggest_int('layer_2', 100, 2000)\n",
    "    layer_3 = trial.suggest_int('layer_3', 100, 2000)\n",
    "    layer_4 = trial.suggest_int('layer_4', 100, 2000)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    \n",
    "    train_loader, val_loader = create_dataloaders(x, y, batch_size=batch_size, train_shuffle=True)\n",
    "    \n",
    "    model = ffnn(input_size=INPUT_SIZE, layer_1=layer_1, layer_2=layer_2, layer_3=layer_3, layer_4=layer_4,\n",
    "                 dropout=dropout)\n",
    "    \n",
    "    gpu = 1 if torch.cuda.is_available() else 0\n",
    "    \n",
    "    print(f'Begin trial {trial.number}')\n",
    "    # wandb_logger = WandbLogger(project='02456DeepLearning_project', name=f'input-{INPUT_SIZE}-{trial.number}')\n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger(save_dir='Project/lightning_logs', name=f'input_size_{INPUT_SIZE}-layer-4', version=trial.number),\n",
    "        max_epochs=2,\n",
    "        # log_every_n_steps=5,\n",
    "        callbacks=[PyTorchLightningPruningCallback(trial, monitor='val/acc')],\n",
    "        gpus=gpu,\n",
    "    )\n",
    "    \n",
    "    hyperparameters = dict(\n",
    "        batch_size=batch_size,\n",
    "        layer_1=layer_1,\n",
    "        layer_2=layer_2,\n",
    "        layer_3=layer_3,\n",
    "        layer_4=layer_4,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    trainer.logger.log_hyperparams(hyperparameters)\n",
    "    \n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # wandb_logger.experiment.finish()\n",
    "    return trainer.callback_metrics['val/acc'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972670e-265a-4bb0-acf1-9beedd9edeec",
   "metadata": {},
   "source": [
    "## Run the optuna experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2704f29-85fb-47fc-a336-65517eb00fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_kwargs = {\"project\": \"02456DeepLearning_project\", 'name': f'input-size-{INPUT_SIZE}-layer-4'}\n",
    "wandbcb = WeightsAndBiasesCallback(metric_name='val/acc', wandb_kwargs=wandb_kwargs)\n",
    "\n",
    "print('Finding optimal hyperparameters using Optuna...')\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=50, callbacks=[wandbcb])\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f'\\t Value: {trial.value}')\n",
    "print(f'\\t Params:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'\\t \\t {key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688c640d-5b81-4341-987b-1ec3fa2a4450",
   "metadata": {},
   "source": [
    "## Findings so far:\n",
    "\n",
    "\n",
    "Input_size = 100:\n",
    "\n",
    "\t Value: 0.8661864995956421\n",
    "\t Params:\n",
    "\t \t batch_size: 59\n",
    "\t \t layer_1: 1670\n",
    "\t \t layer_2: 1980\n",
    "\t \t layer_3: 267\n",
    "\t \t layer_4: 1694\n",
    "\t \t dropout: 0.2469524816724903\n",
    "\n",
    "Input_size = 20:\n",
    "\n",
    "         Value: 0.8385140299797058\n",
    "         Params:\n",
    "                 batch_size: 30\n",
    "                 layer_1: 1339\n",
    "                 layer_2: 1090\n",
    "                 layer_3: 870\n",
    "                 layer_4: 1797\n",
    "                 dropout: 0.10213636974145009\n",
    "\n",
    "\n",
    "Input_size = 2:\n",
    "\n",
    "        Value: 0.5481425523757935\n",
    "         Params:\n",
    "                 batch_size: 51\n",
    "                 layer_1: 1171\n",
    "                 layer_2: 572\n",
    "                 layer_3: 174\n",
    "                 layer_4: 149\n",
    "                 dropout: 0.1737342682313769"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
