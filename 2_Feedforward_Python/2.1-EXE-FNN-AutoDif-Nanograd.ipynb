{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAva8TnYFtFu"
   },
   "source": [
    "# Contents and why we need this lab\n",
    "\n",
    "This lab is about implementing neural networks yourself before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates. \n",
    "\n",
    "All the frameworks for deep learning you will meet from now on uses automatic differentiation (autodiff) so you don't have to code the backward step yourself. In this version of this lab you will develop your own autodif implementation. We also have a [version](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_NumPy/2.1-FNN-NumPy.ipynb) of this lab where you have to code the backward pass explicitly in Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCa7HzwpFtFy"
   },
   "source": [
    "# External sources of information\n",
    "\n",
    "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
    "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search.\n",
    "3. [Nanograd](https://github.com/rasmusbergpalm/nanograd) is a minimalistic version of autodiff developed by Rasmus Berg Palm that we use for our framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SjiIp-TFtF0"
   },
   "source": [
    "# This notebook will follow the next steps:\n",
    "\n",
    "1. Nanograd automatic differentiation framework\n",
    "2. Finite difference method\n",
    "3. Data generation\n",
    "4. Defining and initializing the network\n",
    "5. Forward pass\n",
    "6. Training loop \n",
    "7. Testing your model\n",
    "8. Further extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyXeAA-HuT7s"
   },
   "source": [
    "# Nanograd automatic differention framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6UWKCLKubgA"
   },
   "source": [
    "The [Nanograd](https://github.com/rasmusbergpalm/nanograd) framework defines a class Var which both holds a value and gradient value that we can use to store the intermediate values when we apply the chain rule of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jd4CoEBNzNWS"
   },
   "outputs": [],
   "source": [
    "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n",
    "\n",
    "from math import exp, log, tanh\n",
    "\n",
    "class Var:\n",
    "    \"\"\"\n",
    "    A variable which holds a float and enables gradient computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, val: float, grad_fn=lambda: []):\n",
    "        assert type(val) == float\n",
    "        self.v = val\n",
    "        self.grad_fn = grad_fn\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def backprop(self, bp):\n",
    "        self.grad += bp\n",
    "        for input, grad in self.grad_fn():\n",
    "            input.backprop(grad * bp)\n",
    "\n",
    "    def backward(self):\n",
    "        self.backprop(1.0)\n",
    "\n",
    "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
    "\n",
    "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert type(power) in {float, int}, \"power must be float or int\"\n",
    "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
    "\n",
    "    def __neg__(self: 'Var') -> 'Var':\n",
    "        return Var(-1.0) * self\n",
    "\n",
    "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
    "\n",
    "    def relu(self):\n",
    "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
    "    \n",
    "    def identity(self):\n",
    "        return self\n",
    "    \n",
    "    def tanh(self):\n",
    "        return Var(tanh(self.v), lambda: [(self, 1-tanh(self.v)**2)])\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        return Var(1.0/(1.0+exp(-self.v)), lambda: [(self, sigmoid(self.v)*(1-sigmoid(self.v)))])\n",
    "    \n",
    "    def exp(self):\n",
    "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
    "\n",
    "    def log(self):\n",
    "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDX67D6jzcte"
   },
   "source": [
    "A few examples illustrate how we can use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xk6PeLc3zwPT",
    "outputId": "1e8c6d68-749a-44c3-8fce-0029dcc3215b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=5.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n"
     ]
    }
   ],
   "source": [
    "a = Var(3.0)\n",
    "b = Var(5.0)\n",
    "f = a * b\n",
    "\n",
    "f.backward()\n",
    "\n",
    "for v in [a, b, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmKhYgsY0g_o",
    "outputId": "d86771b7-4166-40a5-8b35-8a46a5d7b1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=14.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n",
      "Var(v=9.0000, grad=3.0000)\n",
      "Var(v=27.0000, grad=1.0000)\n",
      "Var(v=42.0000, grad=1.0000)\n"
     ]
    }
   ],
   "source": [
    "a = Var(3.0)\n",
    "b = Var(5.0)\n",
    "c = a * b\n",
    "d = Var(9.0)\n",
    "e = a * d\n",
    "f = c + e\n",
    "\n",
    "f.backward()\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe3B6uEH140p"
   },
   "source": [
    "## Exercise a) What is being calculated?\n",
    "\n",
    "Explain briefly the output of the code? What is the expression we differentiate and with respect to what variables?\n",
    "\n",
    "### Answer:\n",
    "\n",
    "> To take the simple example: First, we define two variables $a = 3.0$ and $b = 5.0$. These two variables are then multiplied, which gives $f$ the value $15$. Then we backward propagate from $f$. Here we differentiate with regard to $a$, which gives $a$ the gradient of $5.0$ ($b$'s value). Then we differentiate with regard to $b$, which gives $b$ the gradient of $3.0$ ($a$'s value).\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial a} f = b = 5.0 \\\\\n",
    "\\frac{\\partial}{\\partial b} f = a = 3.0 \\\\\n",
    "\\frac{\\partial}{\\partial f} f = 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> For the second output, the same happens. Though, whenever we add two variables, their gradient is going to be one. E.g. the gradient for $c$ is $1$, as the derivative of $f$ with regard to $c$ is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8_Q0t2I3Ruj"
   },
   "source": [
    "## Exercise b) How does the backward function work?\n",
    "\n",
    "For the first example above, make a schematic of the data structure which is generated when we define the expression for f. Then execture the backward function by hand to convince yourself that it indeed calculates the gradients with respect to the variables. Write down the sequence of calls to backprop for the first example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5oi21W4gpeM"
   },
   "source": [
    "## Exercise c) What happens if we run backward again?\n",
    "\n",
    "Try to execute the code below. Explain what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCtpJyr-gyX1",
    "outputId": "86d38af1-1c6e-487b-f4e6-bf29f0bb105e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=28.0000)\n",
      "Var(v=5.0000, grad=6.0000)\n",
      "Var(v=15.0000, grad=2.0000)\n",
      "Var(v=9.0000, grad=6.0000)\n",
      "Var(v=27.0000, grad=2.0000)\n",
      "Var(v=42.0000, grad=2.0000)\n"
     ]
    }
   ],
   "source": [
    "f.backward()\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "> The backpropagation is done one more time for the second example above. Here, we see that the gradients add on top of each other. This is not the desired behaviour, as we want to reset the gradients everytime, which we have to actively do ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8bPVq2VhsP-"
   },
   "source": [
    "## Exercise d) Zero gradient\n",
    "\n",
    "We can zero the gradient by backpropagating a -1.0 as is shown in the example below. (If you have run backward multiple time then you also have to run the cell below an equal amount of times.) Explain what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnyPDQx9lJe0",
    "outputId": "0b86a0c1-3dc7-4f72-e661-649fa7b53c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=2.0000, grad=0.0000)\n",
      "Var(v=5.0000, grad=6.0000)\n",
      "Var(v=15.0000, grad=2.0000)\n",
      "Var(v=9.0000, grad=6.0000)\n",
      "Var(v=27.0000, grad=2.0000)\n",
      "Var(v=42.0000, grad=2.0000)\n",
      "\n",
      "Var(v=2.0000, grad=0.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n",
      "Var(v=9.0000, grad=3.0000)\n",
      "Var(v=27.0000, grad=1.0000)\n",
      "Var(v=42.0000, grad=1.0000)\n"
     ]
    }
   ],
   "source": [
    "a = Var(2.0)\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)\n",
    "\n",
    "f.backprop(-1.0)\n",
    "\n",
    "print()\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "> We see, that when we backpropagate with $-1.0$, we subtract the gradients from the variables, instead of adding them. Doing so several times will keep on subtracting the gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4057_ljNvWB"
   },
   "source": [
    "## Exercise e) Test correctness of derivatives with the finite difference method\n",
    "\n",
    "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to numerically test that backpropation implementation is working. In short we will use\n",
    "$$\n",
    "\\frac{\\partial f(a)}{\\partial a} \\approx \\frac{f(a+da)-f(a)}{da}\n",
    "$$\n",
    "for $da \\ll 1$.\n",
    "\n",
    "\n",
    "_Insert your code in the cell below._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TGil92lSXDN",
    "outputId": "4993ad11-b380-487c-bcf2-c29a12ef60e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=5.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n",
      "5.000000413701855\n"
     ]
    }
   ],
   "source": [
    "# test function - try to change into other functions as well\n",
    "def test_function(a):\n",
    "    a = Var(a)\n",
    "    b = Var(5.0)\n",
    "    f = a * b\n",
    "    f.backward()\n",
    "    return a,b,f\n",
    "\n",
    "for v in test_function(3.0):\n",
    "    print(v)\n",
    "\n",
    "# Insert your finite difference code here\n",
    "def finite_difference(da=1e-10):\n",
    "    \"\"\"\n",
    "    This function compute the finite difference between\n",
    "    \n",
    "    Input:\n",
    "    da:          The finite difference                           (float)\n",
    "    \n",
    "    Output:\n",
    "    finite_difference: numerical approximation to the derivative (float) \n",
    "    \"\"\"\n",
    "    \n",
    "    fa_da = test_function(3.0+da)[2].v\n",
    "    fa = test_function(3.0)[2].v\n",
    "\n",
    "    finite_difference = (fa_da - fa) / da\n",
    "    \n",
    "    return finite_difference\n",
    "\n",
    "print(finite_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We see that the gradient for $a$, is approximately the same, using the finite difference method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pZar5RKaUkg"
   },
   "source": [
    "# Create an artificial dataset to play with\n",
    "\n",
    "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y6yfMAQ8aduj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4YabfD43ajNh"
   },
   "outputs": [],
   "source": [
    "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
    "    # Create covariates and response variable\n",
    "    if D1:\n",
    "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
    "        np.random.shuffle(X)\n",
    "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
    "    else:\n",
    "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
    "        np.random.shuffle(X)    \n",
    "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
    "\n",
    "    # Stack them together vertically to split data set\n",
    "    data_set = np.vstack((X.T,y)).T\n",
    "    \n",
    "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
    "    \n",
    "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
    "    train_mu = np.mean(train, axis=0)\n",
    "    train_sigma = np.std(train, axis=0)\n",
    "    \n",
    "    train = (train-train_mu)/train_sigma\n",
    "    validation = (validation-train_mu)/train_sigma\n",
    "    test = (test-train_mu)/train_sigma\n",
    "    \n",
    "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
    "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
    "\n",
    "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u1oDngHLapIz"
   },
   "outputs": [],
   "source": [
    "D1 = True\n",
    "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Ysfa3FsBavlm",
    "outputId": "26d5d03c-6c26-47ce-cbab-676c99c30b73"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA//klEQVR4nO29fXhcZbnv/3kmmTSTlGbaJjVvRShi9xGIpBTl0OILlXRvRyBUiBzY25ezkb1/uCXo71do1V1G9NAA5whBN9feiPsc3OqWAqUUR2wQVCxcaFuCBRQEKp7mzTZpJ6F5aWYyz++PmTVZa2ateUkmLzO5P9fVK+ma9fLMotzrWffzvb+30lojCIIgFC6uuR6AIAiCMLNIoBcEQShwJNALgiAUOBLoBUEQChwJ9IIgCAVO8VxctLKyUp922mlzcWlBEIS85cCBA/1a66psj5uTQH/aaaexf//+ubi0IAhC3qKU+vNUjpPUjSAIQoEjgV4QBKHAkUAvCIJQ4EigFwRBKHAk0AuCIBQ4EugFQRBySOBQgKZHmmh4sIGmR5oIHArM9ZDmRl4pCIJQiAQOBfA/72dsYgyA3uFe/M/7AfCt8s3ZuGRGLwiCkCPaX2yPB3mDsYkx2l9sn6MRRZEZvSAIwhTY1dnNXXtepyc4Sq3Xw+aNq+kb7rPd12n7bCEzekEQhCzZ1dnN1p0v0x0cRQPdwVG27nyZJW57d4Lq8urZHWACEugFQRCy5K49rzMamrBsGw1NcPLIRkqLSi3bS4tKaV3TOpvDS0ICvSAIQpb0BEdtt/f3nYX/Qj815TUoFBXuFdB/Ff90P6xre4Zdnd2zPNIokqMXBEHIklqvh+6EYH+Zay9fKXmY6u/346uoZ98ZX+TT+94dn/kb6R2A5sa6WR2vzOgFQRCyZPPG1XjcRfG/X+bayx3uB6jmKKBh8DBnv/jPXDLxK8txo6EJ7trz+iyPVgK9IAgLhYM74O6zwe+N/jy4Y8qnam6sY/umc6jzelDAV0oexqPGLft4OMnNxcnXcEr7zCSSuhEEofA5uAOeuBFCsSA7eDj6d4CGlimdsrmxbjIF47/Wdp9aNZC8zeuZ0vWmw7Rn9EqplUqpXyilfq+UelUpNbfLy4IgCIk8fdtkkDcIjUa3p2FXZzfr2p7h9C0B5wXVinrbY3tZbvm7x13E5o2rMx52rshF6iYM/L9a6/cBFwBfUEq9LwfnFQRByA2DXdltj+Gkl08K9hu2gds6Ux9lEXeEWihSCoA6r4ftm86Z9YVYyEGg11r3aq1fjP3+DvAHYPa/iSAIghMOM27H7TGc9PJJC6oNLXDpvVCxEo2iW1dyy/jfszuyngmt8biLaPpAN/e99bk5MTvLaY5eKXUa0Aj8xuaz64HrAU499dRcXlYQBCE1G7ZZc/QQnYFv2JbyMKeFU9vtDS3Q0ML6tmeSpJchz34e+fNOcIWA2Tc7y5nqRim1GHgUuElrPZT4udb6fq31Wq312qqqrJuYC4IgTB3TjBtU9Oel96ZdiHVaOE21oGr3EFhUtSce5A1m0+wsJzN6pZSbaJD/odZ6Zy7OKQiCkFNiM+5s+OhfVfHDF/4v2rQt3YKqXTGVcgdt950ts7NcqG4U8D3gD1rrb01/SIIgCHPPrs5uHj3QbQnyCvjkeXUpF1QTi6kACHtt950ts7NcpG7WAX8HXKyUein25+M5OK8gCMKcYbcQq4FfvHY05XGJxVR1Xg9Xrbp+Ts3Opp260VrvJfqgEwRBKBjWDj3FQyU7qFX99OhK7gy3sDuyPqPKVksxlXG+Q8tof7GdvuE+qsuraV3TOmtdp6QyVhAEIZGDO2gr+R4eTgJQr/ppcz8AITiw5BLLroFDgYwCuG+Vb87aCSqtdfq9cszatWv1/v37Z/26giAIQNQS4enbogVTFfVRmaV5ofbus6M2CQkc04spLTuFstE+qKgn0HgF/q6fWdoHlhaV4r/QPyNBXSl1QGu9NuvjJNALgpD3JATuQOMVtPf/xn6Wneh7A+D2EFj3+cljQiFajwfxDY9YLqOx5qmbVtbRW5yw8ApUlFSw97/tzfnXnGqgF/dKQRDyGyNwDx4GNIHwAP4/PUbvcC8aHS9Oilei2vjeBEoU/rdNx7iL2VK1nHNOW0lTfS2B8jIgeTGyr8g+hA6eDBL4l0mHzMChAE2PNM1JVSxIoBcEId8xBe5AeRlfqVrOmMsaki3FSTb+Nu1LvYyphDCuFChFr7sYf+WyeLA3Ux2eSNpmHNu+aAKeuJHAL/8Z//N+5wfPLCCBXhCE/CYWuAPlZfgrlxFJDNgx4sVJNv42fTbpFzNjLhfty5YlbW89HgSH9HdfcRGERmk/9Jglhw+zWxULEugFQch3YoG7famXMZdzSIsXJ9k4TTrOzE30FbuSjvONa7zFyTN98zn7HIY0W1WxIIFeEIR8Jxa4U83KLcVJDS3w/mswZ9xbjwcpjaQWplSX19j65Wy58NbkYqhIJDrbB6ojTuebnapYkEAvCEK+EzMscwqoLuVKlju+0QEmcwPf8Aj+/gFWhCJobZONibhprfygrSTTt8qH/0I/Ne4KlNbUhML4+49FFTtuD62rrpjTqliQQC8IQiHQ0ELrR+6wDai3r789WdNusyDrGx7hqcPdnHitjbGeTxEZ96I1RMa9vK/vLHzPfdei7Gna56fhwXNoeqQJgI5r9nJwzTY63inCNzwan/H7PvKN6IOgvAaFoqa8ZsZ09k6Ijl4QhILBrko1NHgud+15nZ7gKLVeD5s3rqb5lxttC6L6qOKCseRF0pdK/wEv70SvEVv0Na8HzGSRlJmp6ujFAkEQhIIh0WbAaAV4ycSvor41o/307qrkrdM+xhkjjycVTR0+ZzOefUUWM7MrS56nIhbkwX7R11DRzJXFQTokdSMIQl6Tqhjprj2vc8nEr2hzP0C9qx+XgjrVT+2fH4suyMZa//VRRevw57jp92fyyfPqLM6Tt5U/aimUclr0nU0VTbbIjF4QhLwlcCiA/3l/XKee2KKvJzjKQyU7KFPjluM8nIQ3Otj1kT1s3fny5Aw+OMqjB7qtTbz91gBeHZ6g150cOmdTRZMtMqMXBCH/OLgD7j6b9mf+v5TFSLVeD7Wq3/4cg11JnvOXufbylPoClz1+VtTY7OCOpAKrqBTTKvHRETfHuz7Grs7uHHy53CMzekEQ8guTKVnf0pW2uxhplM0bV9O7q5I6bIJ9RT1HRp+n/Iw9KHeQ0rCHjxzrpX4k1vJ68HD0Ou+/Bn73o3g+3zc8AkUl3FVVR//EO+iQl5NHN3Ji6Cy27nwZIGUHqrlAZvSCIOQXJm8bp4pWI43S3FhHz3k3M8oi6w5uD4HGKyit2YmrJIhScNI9yvaqJVZPm9BoVHOfUCjl+9hdhLq/yYnX2hh+awvhoUYARkMT3LXn9Zx/5ekigV4QhPzCpIG3S6MkFiOdf9k/4Nn0naSK1vb+34ArZDl2zOWifak3+XoNLfClV8AfZNdH9rDup5VJDcANMulANdtI6kYQhPyioj6ugTf84tuXeukrLqJ6cS2tlR/E9/gtMHittamIubEI0Nf5TdvTJ6lqTDl6Q66Z2EvWTK3X4/jZXCEzekEQ8osEUzLf8AgdfzkerUp973XxCtZAuYemUyZoePE2mn60PskW2EklY0kHuT3R68WwaxhuxuMuYvPG1VP8YjOHBHpBEPKLmLdNYiqGhpZ4/t6oXu11F6OVojc0mOQB37qmNdkyQblpPVmUfN4YqdIydV6PVZY5j5DUjSAI+YdNKgaI5+8zqV41fmbS2Nug1uuxzc3XeT08t+XiqX6bGUcCvSAIeYGdj01SUI7l71NVr2Z0Hgc2b1ydlKOfr+kaMxLoBUGYXyQ0+mbDNgKLy1NWwMbZsA2euNGxenVJyZLMzuOAkZZJMkmbh+kaM+JeKQh5yK7O7rwLNhlhKoaK4/bQdPp76A0NJu1eU15Dx5UdSecI/Po2/GXamr6JuClzexiZGMrsPPOQqbpXymKsIOQZhsSvOziKBrqDo2zd+fK8Lb/PClMxVJzQKH3jQdvdbY3EGlrwfeEVPvHuLejQpKf8aO8mhsPJQd7xPNMlZtOA3ztppzBHSOpGEPIMO4mfUZGZL7N6xzy5TUMQmJqRWMdv6zgR3GLZpqv2oEqCWZ1nSiS+mRh2CmC/iDzDyIxeEPIMJ4nffKzItMNwnOwd7kWj43nywKFAkoGYQevJoqzb8dndj5NHN6Ij7qTzfKj+Q45Wx1PC4c2Ep2+b3nmniAR6QcgznCov52NFph3tL7Y7O04mFEMB4Pbgu2hb1u347O5HeKgRz+DVlvNc/p7LefzNx+0fPFPF4c3EcfsMI4FeEPKMzRtX43Fb5YP5IPEzcMqH9w33pSyG8q3y0XFlBwcbv0bH4R583782Ze7b6T599cPXRs/zmYN0XNnBs13PprQ6nhIObyaO22cYydELQp4xbyR+NjLIdPnnXZ3dEPZC8fGkz6pDoWjg3rAtaiDmdM0Mc9+Z3qeUD56pEpN5JqqHzHYKs4nIKwVByB4HGWSiZYAZQy0U8uyntGYnyuQcWRqJ4O8/FjUpc3uiHvBvdCQ/RO4+27apNxUrnR8OaWh6pIne4d6k7dOWXE7hQZiOqcorJdALwjxmtvTyWVeLTiHgrmt7Jm4fULykk0VVeyhyH6c6PEHr8WDciTKKAiZj04gu4U73DdwabkdhF7MU+INpv6cdie0IIbpAm24NYC6YaqCX1I0gzFMSLXENvTzktoNRur6rtkxhsdGsggkPNRIeauTQomtwKbu9rcG8TI1z3fgP6FHLqbNrDTiN3PdUPG/yDQn0gjBPyUQvv6uzm5cC93Pd+A+odQ0w5qmm7G9uyypFkEoF4xjsTJ7wSdsdsDME69GV1Dv1dE08Xg1wU+j/4Y6S70WbexvkIPftW+UrqMCeSE4CvVLq34FPAEe01mfn4pyCsNBJp5ff1dnN3sfu4zZ1P2WucQDKRnsJP/7F6P/YGQb7KS1GpllsNKecKqtfZdGKPQzVHGVxZQVjRzbGW+/dw9W0FT1AseVBY03bxL+3Xs7uyHrUOLRXPZHT3Hehkyt55f8B/jpH5xKEwmEaZfDp9PJ37Xmdm/gxZWrc8nnxxFhWhTmODThSVYumkEGaLRqKlnQyWvFjBkNHAI1yB/HU7MS9pJM6r4f1V9xA8eXftp5n7X9P0tKP6BLuDEeD+f4ll8Tb+vGlVyaD/DyyHJhv5GRGr7V+Vil1Wi7OJQgFwxTK4M0zYW+ZG7dLEYpMzm7Nevme4Ci1ixzSHlkU5rSuabVdjExVdRr/Djbfw5xyWlS1x6KuAcAV4vT3PkvHlV+LbbA5z6kXMPLkNkpH+ujRy7kz3MLuyHrneoFM7/UMKGHygVkrmFJKXa+U2q+U2n/06NHZuqwgzB1ZlsEnmpUdHwmBAq/HjSK5g1Gt10OPrrS/dhaLk75VvqyrTlNhTjkpd9B2n7Qa9YYWym55jd3Nr/Kpsu/y5OJylpx5B8XvuZn73vpcctVqJvfaeBgMHgb05MNgAcz8Z20xVmt9P3A/ROWVs3VdQZgzslSm2C2+hiY05YuKeenWpqT9N29czT2PXc1t+n5L+iZcVEpxlouTuVyMNC+66pB3WiZizY11uCtewv/846lVQZnc61QPgwKf1YsFgiDMFFmWwdu1qAPnRdnmxjrWX3EDd7pvoCtSSQTFiKcmmvM2Ba5dnd2sa3uG07cEWNf2zIzbGZutB5xMxNKmhUykUgUFDgWiZmSn1dNUX0ugvMx6sPlezzP/mdlE5JWCMFNkUQa/q7PbQWuS2qysubGO5savA18HoIxY8dMjTfQN97HEXcWxwxsYCb6f4iWdBJfv4Wu/C/I//7CCrRd8Gd+J4ZznrK3WA414ykqiqpvQ0ew06rF8et9SQCWL7Y2Z/djEGChFr7sYf+UygMkKW/O9noIktFDISWWsUuo/gY8AlcBfgFu11t9z2l8qY4UFQ4aLf+aqUTMKuPtT52ZcIGVX5akjbkLB83B7D1htB5Qbf/8AvqFgwhV1VP0ylwuVpsXVpvpaWy96l3IR0ZGk7TWhMB3vFCWPfwq2DfMNsUAQhBwxE7YD6c55+paA7Wwe4O22zHPnTr4tWiuUSr5CTShMR1eP/ckyDILTabbtiMliIVBehr9ymaUtYGlRaVI6x0ChOPiZg/bnzXPVjVggCEIOmAnbgcRznjf0FOfv+jz68QFULNjUeittZ/R1WXrMO6tZ7B8jfcVFttsBq2rFIThOyT4hE0x5c8MDp32pl77iIqrCmi8d6ebe5cvoLUpO6aTV/+dRYM8VshgrCCZS2Q7k4pyXufbS5n6AOtUfNeeKSfzued8b8QXMy1x72VtyI4cWXctT6oas5H/OQc7WUIbq8ITtdojOpJtOmaDhxdtoOmWCQLknSZLotFC6/YVvZTxmWxLy5r7hETq6enjpT4d5uquLTwwP0zowQGnE+gDLdqF3oSCBXhBMzESbvrVDT8UC9zV8y/2vSZWshEY5/61vs33TOXx28W9pcz9Avasfl9KUjfYyuvOf2Lf73zK6VmvlBylNSMe61SLcwxcmq18imtbjQdvzGOmSXncx2rTQGSgvs8z0nd4gguNHpqbuMapbBw+T+HCKaCwGaL7hEfz9A6wIRaLLCu4V89Jxcj4ggV4QTOS8Td/BHbSVfC8WuKFYJS8eAjB4mOZfbsQfvifpQeDhJLUH7kwfOA/uwPfcd/EfHaAmFEZpTU0ozDeOD9J50cXc8eFvUOOuiG/39w8kWANP0r7Ua8mJA4y5XLQv9cbGG02tOL1B6JCXmx56KWM5Z+BQgKYfrTe9PZQBmgjRAN8VqbR9J/ENj/DU4W7eea2NgT9sJjR4btprLUQkRy8IJjZvXG3Jp8M02/Q9fZvVadGBiAaXnfQvRg0DFtdKW568BUKj+EIkB/AnbsT3/mvw/enN5KIhA88yKCmHwS7H3H18eyy10rqmlVt+9c8WNY+OuDl5dCNgXeMA+25Pljy/jUyym0rWj9/L3pIbbZ0ue/RyINnZU5hEZvSCYKK5sY7tm86hzuuxtR3ImgyKcRJTEnb06OX0BEcnC4QebKDpkaZJK4CDO2D0mPMJQqNw4P84B3mA0eNxs7DqxbW2u1SHJyz6dN8qH57Bq4mMe9EaIuNexno3xd0pIRqAn3n4O6x97EP8evQKfl1yI+cNPcXWnS+zq7PbPs9venuoVQPUeT3cFW5hlEWW/cxmZzC9FFshIzN6QSBZIviVlhw1nnAq0lFFRHSEnshyatP4sY/oEr7suZDFp32DLb8ejm+3KFwycavUzguv8bHGsDU6i0RoPVmUJLn86oevZevOhqRFbANjAdpISdWrftrcD0AI7tpTwjs1DjbJsbcHVVHPc1+6GLgYDjbC07cRGeyiJzJpdmYw5RRbgSOBXljwzJhEEJyrYy+9lzN+VI4Gx5SE1tCtK/my50J+X/1qsgskpgYhmZTxqyLQ0fy3IVWMt/Eb15Yq0my6LhlvOzc99JLtZW8u3pG07lCmxrm5eAcXBddz5nuqbbX/1eEJcLmt1a0xeeRuQ7IayVGKrcCR1I2w4EnlpQI4p0syIYVvuzH7vDPcwogusRwWLirl6+6buGj8Xl5b8WfbIG/QN9yXvozf7YHzPktgiddGTbOcwLrPJ+nLfat8dFzZwcHPHKTjyo6UD73mxjpHzb/TG0utGqDW66F1TWuSUqg0EokqghadYqt7z3mKrcCRylhhwdPwYAPapqBIodh+0fYZaxxtLqS6zLWXm4t3UKsGGCuztgN0Gp9BTXkNHe+9LvnNwcbOoOlH6+kNDdqf48qOnH0fM3tLbqTelRzsu3Ul+5qfjS7I3lVL+9IK61vG8AjTafpdiEhlrCBMkepyh9RBefXU+qlmiNn864ngeg6UXWJrt+A0PoBiVRwtEDLGkqa8vy80ZHuetP7wDiRaO3zyvDp+8drReOOUE2Nh7gy3WHL0AKMsoue8m+Pf1Ve8DF/XwjQcmw0k0AsLnlQdlrb+eqvtMbkKjJs3rua5LRenHd+WX2+x/WxxyeLJB06q8v6Yx0v1KRO2BmFmPXymXj92dhGPHui2pFCi5ypl6xBsLXmYd9HPqKeaO0Of4sHn66n9/TPR82fh9Clkj6RuBAFnYy4nk7CppDoSA+Nlrr3c4o6ma1Qag61zHjzHdntKAy8Dk2ujk0GYkYqyS7943EW2+W8nx806r8fx4WU+f/GSzmirQXcQb8kKttZeiK/zsbw1HJsNJHUjCNPAqcPSlPup2mDneRNPZwweZuTRL3Dn7lc513d9UlCtKa+xfeAopQgcCtiOPf7wOtFD9buW0npcJRmEVS+utahpUnn9JI5pKnYRxvmLl3RSWrMzvsg8GDqCv+tncPkdYmEwA4jqRhBSYO2nCjUTGn9vN77Hb3E2GzP8Wvze6M/YfuYA6CQ5vG78B/FCIjOta1opLSpNulRER/A/709SAhmS0d7h3iSvGsMg7ODbh+k43IPv+9fGx5lR8I59v7dKr2VvyY1c5tpr2VeDo/WBcR67puFmpZOQWyTQC0IafKt8dLz3Og52HaXj/x7GNzzs3Fg6liYJhAdoqq+hYSk07fMT+OU/W4p5UkkO7dwyjQeOSyX/L2sXINNVm0ZRSY2yP7P4t/bj8nrY1dnNo1//FJFHPw+Dh3GhqXdFi58Sg71hfZAY7I17MOWm4cKUkEAvCJmQqrE0k31Zux7ZSqBEWbXqxUX4336Mpg90x62Ie3Sl7WUM3xa7mbVvlQ+nNbXEAOkUMCc9bGwaF4ZGudn9UHyMmPbsDo7yi4e/wxWRnyXZNRjFT4nYPbCMfrI65LUdX6ZNw4XskEAvCJmQorG0scDYHRylVvXbOz8qxXPH/iNe5JPOt8WplN8pECZud9wvPBEr3rJ/YJSO9vHJ8yaLn8yPg83FOxw9eWrVgO32xAeWUehUNnzptJuGC5kjgV4QMsFJz11Rb1nA7NGVzs6Pw300N9bx3JaLab99O55N32HEU0NEK7oilWwJXcfuyPqUpfx2uXq7AOm438X/M2pcVrHS9vw9keU8eqCbzRtXU+f1WB4HqTx5jDeRROweWM2Ndey76eaobXJ5DQpFTXmNeMnPIKK6EYRMSKHz7vlRdFvxkk6aVyxHY+/xnjTLbmihrKHFoluvS9Oj1smDJjR4LuvanjFp38/Ff6Hf2avG5vsYbxSjkYn4eMz06EpbT56Ijto4JCaD0nnPOCmdhNwjOnpByJTExtJnNsEbHUQGu/hB2Qr+V1UZEZe9g2OubBPsyEb7buHgDroe2UqtGqBHW50gFdHZuFknnyQJJRrk/2PiY7Spz1uqYnPVVF2wIjp6QZhpzJWnpiIkF/CDZUX2QV5rasITtJ4YxXdiOPnzHJCN9t1CQwuf+ql9U3IjUJsfILsj61GhaK6+1jVAH8u5I9TC/iWXsF2C+rxGAr0gTIUEFY5TXl4BHV090b88cWP0Z46rPafT5zYxmBcv6aR0xR6G3IPc91Y1V3/07+j4bV18lv7Rjf9EfeN2AGqBVKr3TK0UhJlHAr2wcEhMvSSW2Js+D1TVR6tHQ0P2XuwJKpzqsIOHTNg00zbkmDkO9IkpFvP2dJiN1Y5Enqe0ZifECpl6h3v5ydi9+FuyTznZ+eAYLQUl2M8+oroRFgZGqiWhQChe8GT6PFDuwV+m6Q0NotHxRiSW6tMEFU7r8SClEWvj77inuplMGoRkiaFNN5NNEw5DCXT6e5+NB3mDqVarpkonCbOPBHphYeBQ8DTy5LZ4oZPxua0OPjHgbdgWVd3E8A2PsO3oMWpCYZTW1ITC+PuPJTfpngHb3ZRNOBzsGOxwLLKaQrXqdNJJQu6R1I2wMHCYSXtGevk1V6BMhUCpdPBxYumXvp1fYYXup0cvxzfcz6Uj9tJKIOe2u2lz4KYFY2DyLcY0fjOpfPmzZTrpJCH3yIxeWBg4zKSVApfCEugteXUTdjr4C8baWXXyh6wfv9fR1iB6/ZUE1n2epj8+MLWWhAmYq3E1Dt4yaWwbEnEyThsJjWQ91ummk4TcIoFeWBgkpFpSYZtvN6pPTamQkTv+istNZl52vV9xe2DTdwlcfgf+rp9F3SSd8v5ZkFEOPIVtgx2GcZp3kde6+/hg1mOVnq7zCymYEhYOP/ky7P93nHxeALQGjeIH5Sv4Qf0K+kKDVE9oWgeO4YssgvETMDFZMDSiS+LWBUC892udy9pMxLGBSShMxztFWTfZOH1LwPZbKOBPbTGFzN1nxxafE6hYGbVBcCCXzVaE3CIFU4KQjjc6SBXkIdqwev34vdR5PDx3dn+CTUC04ClQXjbZuCM8wd8ee5Tdx6KBfndkPbvH1/N2m1WOmNJNMk3u3I6McuBTbM+Xy0VZYX4gqRthYXBwh/3s1oTh9RLPJdvkuI1WfHELYncx365yU7ykM75Pnc2CY0o3SUiZO7cjoxx4Qwtcem/MwExFf156b9qHSaYOmUL+IIFeKHwM9YkDGuijiq2h6ziw5JLJXLJNLttWeulysahqD+C84GjrJpmos89CY59xDryhJZqm8QejPzN4Y8jUIVPIHyR1IxQ+duoTA7cHdem9VDe0JJfzV9QnvQU4Wh24gymdJy2ukyd6qA5P0Ho8aNXZOyiDnBqXNzfWzcjippNDpjhN5i+yGCsUPn4vjrn5Td91nuUm6tCBppW19BYnz4+yWqi0OS9uj21axej9mticXLzbFyZTXYzNSepGKfXXSqnXlVJvKqW25OKcggCTLfpO3xJwbDidFs9S++0VK1OnMkw5bo2ijyqWHlkL0+2MlEXu3Lb36xw00Q4cCtD0SFNOagCE2WfaqRulVBHwL8AlQBewTym1W2v9++meW5h55rPDYE6MsQ7ugJPvJG8vKsmsSrWhhV0T6ybHMQbFOurwqNyD1Ew1rWG2PE7BfFDAJL5VGDUAgLxV5Am5mNF/AHhTa31Iaz0O/Bi4PAfnFWaYjKorZ5I0Piw5McZ6+jaIhJK3lyzOWMqYOI7wUCMn3tzCkt576LiyY0aD3XxQwMyXtwph6uQi0NcB5hWrrtg2C0qp65VS+5VS+48ePZqDywrTZcYdBlMF8nRukuTIGMtByRIZPW6bDrJLUcylQdd8UMDMh7cKYXrMmupGa30/cD9EF2Nn67qCMzMawFIYau2aWMcFj3+Fahx8WGIz7ZwYY9koZwB+ULaCsro2Bt1BvnbAy++OX8/a05bZpigqq6/iaN9Z0xvHFJkPCphcmp0Jc0MuAn03YG4pXx/bJswiThK8VMyow2AKW+CtJ+7hVdfRaL1+IsYM/OAOnlLbKF3UZ+lnmrUxlk116M6yJfyvqjJcrmB0gzvII3++m58fKbdNUVSs2INnoMHy9lO29HeolU/T8OBNMx5857qJduuaVlvlj+jq84dcpG72AWcqpU5XSpUAVwO7c3BeIUOMxbJsDbNm1GHQIWVSOtrHaGjC2emxoj7+NlA22otLaepd/bS5H+Czi3+bvTFWgsKlK1LJ7ctqkvu7ukIETwZtTzEUOmopTqqqfpXSmp0Mho7kxKBsvmOYndWU16BQ1JTXiLwzz8iJjl4p9XHgHqAI+Het9f9Itb/o6HPLdEyoslLdOLTisz3HLzfapky6IlEvmctce2lzP0CZmjQIi2vJn77N2a6gYiX7zvgiN/3+zCkphda1PcNgdavFljgdkXEv3oGvx68jpl/CXDGnpmZa658CP83FuYTsmc5iWcbVlQ45931vH2frvncnSSDrzv8i5798a1JR0AP6b2E8av5FCG4u3kGtGuCIqqT60tujM/Cd1zuPY/AwZx/4GueFrqOb9VlLLjdvXM3XDnjBHUz6rKKkgpMTJy0pCh1xc/LoRrqHJq8ji5NCviFeNwXArEjwHHLuK1+8y1a5c9Pvz7QtCjrXd308XbQ7sp714/dyVuTHvHD5rybljmna7XnUODcXT6pzslEKNTfWcdWq622LnrZ+cGs8RYGOzuTHejcRHmq0XGc+SB4FIRvE66YAsFss0xE3x7s+xq7O7vQz3YM74MlbCLjGJu13S7y0XrB1Mg/rkHNfofttt/cER22LgppjP1Omi+zsdROoU/1c5tob94HPRil068V/x9pDyxwXr32rfI5+7z3BUb4ji5NCniGBvgAwAtT2F75FcPwIOuTl5NGNnBg6K31a4+AOePwLBEqL8Vcuizsz9oYG2fLrLXQe6eRrF3zNUaZ4RNkvqmqi+XC7/HnadJHxcEiRq1cK2twPQCj6ZpCtUiidkiWVIsm36mJATL+E/EFMzQqIdW3P2AanOq+H57ZcbH9QrAtRU30tvW77537bRW34TgzbGnHtO+frfNqUowcoXtLJoqo9KHcQwl7OW3INbx5aPTWbBTsDMBNdkUou0f+S8zZ1ifYLEFUkSTs8YS6RDlNCPH1htLOrVf306EruGmphV+dq+3RJLCXjZL8L0Zmrz1CTJKhuzm9oYfvKqOqmOzhK8ZJOSmt2olwx2wF3kAPD32UssglNo/3iaUzNowe7+AuVbB+/iv1LLomNMTa73/l527HVugbYfnnug69xvvnqAyQI2SAz+gJiXdsznDf0VJJscZQS/jlyPY+MXxjfFp+dxmSQqWb0CsXBzxxMe/3TtwQoO6MNV0kw6bPIuJfhtyaNTeNvGTYzdqMP61NFH56cQWfa/9RBAioIhcCc2hQL84PNG1dzi3uHVZsOeBjnJn4MRNMq5We0UXTGZr564Go+NdbIOMXRTkcOD/1ENYmTdXCt1xNN19iQuD2+eGqj5imLqWosapoN28DtIVBeRlN9LQ2nraRpZR2BxiuAmEfNj9bT8OJtNJ0yQaDcY+ufIwgLEUndFBDNjXXoxwdsP6tVA0lpFeUO8mr1q/xt38f51xO/oGXRO+xYcgrmaqJENUkq6+BUGnUd8lrHYyyeOqh5alX0e8QfCA0tBI69jP/txxiLja+3uAh/18/ofGERj7/5eFQFE+vj6q9cBhDt4PTYP8TPIQgLEZnRFxjKQYPeo5dHF0iN3LmxvyvEq5Vvs+bkv/Gzkf9N24fuSFnqnsrx0kmjbhQdGVhsFlKMF6y+O+39v4kHeYOxiTEe/uPDyR41LhftS73GAGDXDTKzFxYsMqMvNGw06KO6hDvDLSj3T2wPMdIqPcFRW9mh2eLAaUXHmHnbadTXLfs7Ov5SRw82i5o24x2JjTfRd8ep8jSiI7bbe80LzJGQxRlTEBYSEugLDZMGXQ920aOXc0co6vxYHtqLslkoNdIqdlp0O5mhHeZj7R4WtzqoOxPH+xcq2R66igNLLmF7gsrFyS7XpVyOwT5QXjbZgNshTSQIhY6obgqYRF19kvQRWBTR3Hp0gDXDZfScdzPnXxbNZxuzeDtdfiKzpS+3a5RdrIopKSphJDxie0xNKExHV0/0L4kKHUHIM0RHX2Dkopdroi1AeKiRMWBR1R5c7iDV4TA3HQ/iGxkBNULdy7fCaUutPVJToGBqBVBTlD8mNuEoLSpldGKUcDjseEy8PsDlzqxHrCAUIDKjn4dkU5WZ6oHgVCkLsLfkRupdNj41FStZd/LeePGTUeFq2Cp8/MRwtBjLNYArW526XZWrYU1sOkcmTVQChwJs+fUW0lETCtMxMAZ/c4fk54W8Z6ozegn085CUVgYf74/PiEc81Wwb/qSlEEoR9Zmp83r46F9V8dBvDxOKWP8bFy/pZNWKH0bNy8ITtB4PTuaxUZw+9kOKbNI8rkgRtx4dYNPIkHVgnmWZBdIMip7s0jOlRaV8ovZGOn5bF3+gqVP/B4OhIykvV6rc+N8Zx3dUiqeEwkAKpgqEXZ3dSUH+Mtde9pbcyN7RK6Je7bGG2mWjvdym7ucy1974vkZI7w6O8tC+w7iLrHJEI0/f6y5GmzTngfKy6A4V9dR6PbZSzIhrgltXVNBUXzu5P8DoscwKk5wWQ03b219st23n9/Ch++mOqX66g6MEx1MEea2pCYXx9w/gO+rcfFwQFgoS6OcRRsrGjNGJqd7VH6tjss7OyxK82c2EJjQjIasaxS6AxzXnbg9s2MbmjasdK1yxezjAZGPvVDj5zJu2OzbvKA5SvKQz/tfEAqzJDzRtRwfo6P4LvqGg9bNMxigIBYgE+nmEXTHSzcXJlgaJGFWkmeAUwPuKi+K58ubGOrwlK1Kex1KQZJBOvhizMbAQe7gYODXvUApKa3bGg/3JoxvRCYVZaM2nht7BN65BOywki8RSWIBIoJ9H2DXPqFWTC6YWnxdT+sSoIs0Ep5lw9eJaS/566wVfprSoNOW5khwvlSspNWLxxflpJfvO+bq169T7r4nOsv1euPtsWllOqcO6kXKFWFS1B4gqiDyDV8eqeKFmQtN29BhfmzjF1NnKhjTdqwShEBF55TzCrtlFj66kXvUTKC+zNgaJpU9O6mJ+Gcx8gbFs+FLwPJy2O5JZymhXpARQHU6YNeuJaB4c4k3DE31xPr3v3WzftCeqDLLpQ+sbPAzlZWypWo5dB2/jjcTjLuKrH76W5sabnb+sncJHJJbCAkRm9LOAk9tjIps3ro73UzW4h6sJF5XSvtQbD/IGYy4Xty+ribfTAyhSCgUsLXPjdlkDpREcjb6oTn42Br5VPjqu7KDtorak2X2pctMaHEo6xpwHT+WLA9j3oSVqRFaT+BCJoUNe6rye9AVaDS22PWtFdSMsRGRGP8Okcnu0a7EH1mYX6zfeQHHR++l70X4Rcax4MlAmau2dNfZ1WbW9SyxUimvbv3+t/QGxPLhTH9f49hT58tbjQcsbDETfPPwbtsRb+aXF0pKwa3IhVoK9sMCQQD9DGEU/vSd6cZ3qpfjoRsJDjYDV7TGR5sY63BUvxYPqfW9V417TSvXiWnufl4mljhWqaXuzZoFtj1WHPrJGHjxV39WUx0Nc1x9vVr64Nvu+rDapIXNqSRAWCpK6mQGMop/e4V5Q4CoJWhQj4DzbNR+r0fQO9+J/3s+H6j+UnD4pKmX7R2/hT20+ntty8ey3uUujorFLRVkcKe2ON+EbHqHjL8c5uGYbHVd2ZN982y41JBJLYQEiM/oZwK7ox1CMGLN6Y1abWO4/Gh61LRh6tutZ/Bf6ba0BMrEMmBFMqZFA+Bjty5fRV6So/uMDtC4up7kxOoa79rzOkcjzeN7VgS4ORt9SKlrxJaZWKurhzCZ4oyM3rQAzKNAShIWAWCDMAA0PNqBtnNu1hhOvtcVz6e6Kl5LK/Z1w6tvqZBngtMA6E6Qbg93nbrWIomMt9PedNXONtzPtMysIeYJYIMwjnIp+EhUjdjP/bM/pZBnQ/mJ7doN2IBPFULox2H0e0icZKX8ibmmwdefLjmqkKZNBgZYgLAQk0M8ArWtabfPp/+2cjZS/p41tB/+GpkeaHPXpidjp3A2cLAMcrQQy4eAOuPtstN/L+bs+xHlDT6UMyOnG4PS5uUrXIrvMFSKxFARAcvQzgrXYqA8V9jJ0bDUPhXZCzGcmVZCvKKmgzF1mybmHBs9lXdszSVJJp65LTm8AaTEpVRRQp/ppcz8AIdgdWW+rGEo3BqfPE6t0nRaop0VDiwR2YcEjM/oZwrfKxw1n/G/Cb97J0Bu3ULz4tXiQT0VpUSlbP7iVjis7OPiZg3Rc2UFo8Fy27nzZ4t5ozKyd3h6c3gDSYqNUSTROSwzI6cZg93liw3AAl1Jpi8oEQcgemdHPIObKUEc3yAQuf8/lSYuoqSpMn9viUMw01YVYB0WK2TgtsbesY0FVbHvi50vcVRw7vIHw0Pst55mICQNSFZUJgpA9EuhnEPPMV4e8to25E3m269mU5zHTHRxlXdszbN54Lh1Xdkx5nBYcipgM4zSLDt6EbUFVis/NVbsupeJB3iBVUZkgCNkhqZupEluwNFwX7RpamGe+tra6NtgtXCbOoM3kXLFio1QZZRF3hVsy85jJkObGOp7bcjF/avMRcZD4zkjOXhAWIBLop4KxYDmYunuRuTI0PNRIKHge6coWqkMhuradgf+bt8aDt12FqZmcKlZslCqeTd+h/fbtaatvA4cCND3SRMODDTQ90kTgUCCjSzo9yFI94ARByJxppW6UUlcBfuC/AB/QWs/bKqhUTbSzJlVpvUnhkWhStmjJ6+hk5904pZEIrceD1LtGuDl0H9seCwM3WM7j1Ow7p7PfKShVEouiDOsGIO16weaNq22boduliARByJ7p5uhfATYB/5aDscwY2ThIZvRAcCytt+a2A4cC3PdWO+/U9HHme6rpHT5uf5zW1CQ06S5T49ykf8yn9myIm5M1N9Y5Ng6f69lvqqIpI9A73Vs7184ZqZQVhAXKtAK91voPAMqmQcR8IpVqxRxMMn4gOLouqmj6pqHFdobrRMVEtK/r1qrltC/1xgN+rRpg7dBTcPeNce+Xe973RT69793zbvabrmgq3b3NpdOmIAhWFkSOPq0veoy0jTIMNmwD7B5uOu6MmI29wTtFLnrdxeiExtvHdTltJd+zrAWc//KtfP/8P1Pn9UQLmnK4QDodnAq0jO0Z31tBEHJO2hm9UurngN3/xV/VWj+e6YWUUtcD1wOceuqpGQ8wF6T1RY+R6QOBhhbY+Xn7i8XSOtlYEEQS3ojGXC7uWerloRNDeDhp3Tk0yvlvfZvntsy9KZfZNbNiUQXFqpiwDsc/NxdNZXxvBUHIOWln9Frrj2mtz7b5k3GQj53nfq31Wq312qqqqqmPeAqk9UWPkZX6w6b5dKC8jKZT6znnwQa0w6qrS2X2EtVXXIyXE/YfzgOb3UTf/ODJIEopKkoqbFsUirJGEOaOBZG6aW6sY/umc9KmOzJ9IABJevNo8+7l9BYpQIOKkKilLC0qJaIjGY1ZTSylR1fafxjr4DSX2DpSRkKUucvi1g1mtU1W91YQhJwyXXnlFcC3gSogoJR6SWu9Mc1hc0Imi31ZqT8Smma0L1/OWEIzbpTCpTURFGpiKf6LbokZnaV2rSwtKiXY3cSd4WHa3A9Qpsbjn43oEsrmgc1utq6ZoqwRhLljuqqbx4DHcjSWeUHKB8LBHdZuSBu2xRtY9D54ju0hGnjy0AgXjbfh+/voDDepCYfLTVlxGUPjQ3GfmNu7POyOjEIIbi7eQa0aoEcv54GSv8U/D9wY0zlW2nW9am70SWAXhDlAvG4yJUWj6cDicsfDqsMT1KqBeC46nQGYQWhjVI64O7Se3ePrgWiqY7vP/oEy27SuabXtKtW6pnVaxVOCIOQeaSWYKSna0jWtrLVPx2hN29EBzj1Rxr7mZ7Oezea0mncGcOpV69RUpaa8Jnfma4KwAJlqK0GZ0cdI22DbQemiB7voXarsZfXAxcMTvHLezVMK0PO9iMjJsXJGul4JgjBlCiLQT3fmm1GqIYV9byTkxWVjQVwTAc+m73B+rFI2Z57x85ycd70SBGFa5KW80uySuP5HG/hKx4O23ZcyJaMG2xu2EU7okjSiS7gj1GJrQVxaVErrR+6w2CEYmnPjQZKpu2O+kfOuV4IgTIu8C/SJQXMwdATXikcoXtIZ3yfb0nrHVMOJnkm/eeCb6h/pilQS0YquSCVbQtexO7Ke8FAjY72biIx70Roi415LsVBGD5ICwrfKh/9CPzXlNbbFU4IgzC55l7qxC5rKFWJR1R7CQ43xbdmU1jumGsITmP3mj498jvWRe23PER5qjF+/zuvBt+ri+GcLMWedruOUIAizR97N6J2CY2JPVrvS+l2d3axreyapAbVtqiHmDR8nNMrWkofTjs+u2jOd4ZcgCMJMkneB3ik46pA3/rtdsDVscu1y+dZUA9SEwvj7j8W94Q3eRX9SGb/bpVha5k5prSA5a0EQ5pK8C/R2QdOtFlE2fGnKYJvOJte3ykfHlR0cPKbp6OpJCvIAqqI+yTPnrqveT2dzkD+96xaeG9tE8y83JrUUlJy1IAhzSV7l6A2J4tjEGC7lIqIj1JTXZCRVzNgmN5Uz5IZtNDckaNtTVMya2/FJzloQhLkib2b0ZrUNQERHKNWa1j+9gu/xW5Jm0YlkYpO7q7ObPhwcIz3L7PuopuofKwiCMA/Im0BvK1FUivalFZOz6BTBPp1NrpHDv338KkZ0ifVgtwf+5g77Ezv2j517z3hBEATIo0DvKFEsjgVvm1m0ubDqvrc+x9UfPeroSW/k8HdH1rMldF1cL99HFVx6r/1sHpy94eeBZ7wgCALkUY4+tdY9hmkWbWdr8JOxe/G32C+CmnP1uyOTjpEK+FNDitz6hm3WHD1E3wDmgWe8IAgC5NGMPiOtu2kWnW016pRb3TW0RGf8FSsBFf2Z6g1AEARhlsmbGb3Vx72X6vAErceOT8ogE2bRTqme3uHemOWwqXlIQwubN65m686XLRJMI4ef1jStoUUCuyAI85a8CfSQIFE0uj0xagnYBk6pHrQmEB7AZ7I2AGhujB6bGNABywPAKLSKHjN/LYQFQRAM8qfxiF0bv4RZtHnmXVn9KmNL/8P2VDWhMB1dPZMbKlbGWwIm4v/mrVw3/gNqVT89upI7wy3sjqynzuvhuS0X2x4jCIIwExR24xGHoqR9bx/n852nExwNJR1ytO8sFntB2TQEiSt1YkQGu9jd2Z08Qz+4g5tD91Hmijbnrlf9tLkfgBA8EVyfi28mCIIw4+THYqxDUVLtgTttg7yB2f/GjEWpA/RElvOlh17itASzM56+jTI1btm3TI1Hm3WnW6QVBEGYJ+RHoHcoPqphIOVhtg1BItqi1BnRJdwZbsFIYFkalzhct1YNJJmmCYIgzFfyI9A7FB/16OUpDwsPNeIZvNpqJnb6Fbz/nbKk5iFm4mZnDtfdtayG+976HA0PNtD0SFPBdooSBKEwyI8cvU1R0iiLuDOcWtLocRfx1Q9fS3PjzZbt6174KN2xAqniJZ2UV7Wh3EF0yMvJoxsJDzVGC6iuSb5uYImX7Us9jMUUPbb9ZQVBEOYR+TGjtylKemXNN3iSixwPcbIrhknfm+IlnZTW7MRVEkQpcJUEKa3ZSfGSzmgO3ua67dUrGdPWdYFCbgsoCEL+kx8zekgqSjofuGtlN/7dr8YXZJeWubn10rPS6tuNz7e9eDvaZQ3ayhWidMUeNp/3Odvr9j3YYHvOQm4LKAhCfpM/gd7ApKdvrqin+YpkPX3ifna6++bGOrYdDNpeQrkHHR8Wjp470hZQEIR5Sn6kbgwMPf3gYcxNu5PsiTPcb0nJEtvL1KQI2tIWUBCEfCO/An2mTT4y2C9wKMBIOLldYLEqThm0pS2gIAj5Rn6lbjJt8pHBfu0vthOKJBdbLS5ZnDZoS1tAQRDyifwK9BX1sXSMzfYs93NaPB08OWi7Pa2DpSAIwjwlv1I3G7ZF7YjN2DX5yGA/p8VTu+1Gm8Hu4CiahOpZQRCEeU5+BfpMm3xksF82i6pGm0Ez8epZQRCEeU5+pW4g8yYfafazNjLpo7q8mtY1rWnbDGayXRAEYT6Rf4E+Ddnk0tMuqsa0+G+VdtETWR73ojcQB0tBEPKBaQV6pdRdwKXAOPAW8DmtdTAH45oSRi49J92gTB74LqDeNelFvzuyPt5mUBAEYb4z3Rz9U8DZWusG4I/A1ukPaerkNJduo8U3vOhT+egIgiDMN6Y1o9dad5j++gJw5fSGMz1ymkt30OLXuwakhaAgCHlFLlU3/x140ulDpdT1Sqn9Sqn9R48ezeFlJ3HKmU8pl+7gRe+4XRAEYZ6SNtArpX6ulHrF5s/lpn2+CoSBHzqdR2t9v9Z6rdZ6bVVVVW5Gn4BhP3yZay97S27k0KJreG7RjdzzvjeyP1mmmn1BEIR5TtrUjdb6Y6k+V0p9FvgEsEFrrVPtO9M0N9ZRd/gnnP3i9/BwEoA6+ql7+VY4bWlmskwDY98UDpiCIAj5gJpObFZK/TXwLeDDWuuM8zFr167V+/fvn/J1U3L32bb2B31U8V/H2sW+QBCEvEUpdUBrvTbb46abo/8OcArwlFLqJaXUv07zfNPHYRF1he4X+wJBEBYk01XdvCdXA8kZDoZm5kbihuRSZvWCICwE8svrJhNsFlFHdElSI3GxLxAEYaFQcBYIiYuofVRye+gqi3UBiH2BIAgLh8IL9GAxNHuhs5undr4MkcmKWbEvEARhIVEwgT5wKGDrRGnk4aVpiCAIC5WCCPSBQwH8z/sZmxgDoHe4F//zfoB4sJfALgjCQqUgFmPbX2yPB3mDsYkx2l9sn6MRCYIgzB8KItA79X912i4IgrCQKIhAn03/V0EQhIVGQQT6bPq/CoIgLDQKYjE2m/6vgiAIC42CCPSQQf9XQRCEBUpBpG4EQRAEZyTQC4IgFDgS6AVBEAocCfSCIAgFjgR6QRCEAmdarQSnfFGljgJ/noVLVQL9s3CdXCHjnVlkvDOLjHdmqQTKtdZV2R44J4F+tlBK7Z9Kf8W5QsY7s8h4ZxYZ78wynfFK6kYQBKHAkUAvCIJQ4BR6oL9/rgeQJTLemUXGO7PIeGeWKY+3oHP0giAIQuHP6AVBEBY8EugFQRAKnIIK9Eqpq5RSryqlIkopRxmSUuptpdTLSqmXlFL7Z3OMCePIdLx/rZR6XSn1plJqy2yOMWEcy5RSTyml3oj9XOqw30Ts3r6klNo9B+NMeb+UUouUUg/FPv+NUuq02R5jwnjSjfezSqmjpnt63VyM0zSef1dKHVFKveLwuVJK3Rv7PgeVUmtme4ymsaQb60eUUoOme7tttseYMJ6VSqlfKKV+H4sNSU01pnR/tdYF8wf4L8Bq4JfA2hT7vQ1U5sN4gSLgLWAVUAL8DnjfHI33TmBL7PctwB0O+52Yw3ua9n4BNwD/Gvv9auCheT7ezwLfmasx2oz5Q8Aa4BWHzz8OPAko4ALgN/N4rB8BfjLX99Q0nhpgTez3U4A/2vx7yPr+FtSMXmv9B63163M9jkzJcLwfAN7UWh/SWo8DPwYun/nR2XI58GDs9weB5jkaRyoyuV/m7/EIsEEppWZxjGbm03/fjNBaPwscS7HL5cD3dZQXAK9SqmZ2Rmclg7HOK7TWvVrrF2O/vwP8AahL2C3r+1tQgT4LNNChlDqglLp+rgeThjrgsOnvXST/h58t3qW17o393ge8y2G/UqXUfqXUC0qp5tkZWpxM7ld8H611GBgEls/K6JLJ9L/vJ2Ov6Y8opVbOztCmzHz6N5sJ/1Up9Tul1JNKqbPmejAGsZRiI/CbhI+yvr9512FKKfVzwK7r91e11o9neJr1WutupdQK4Cml1GuxJ3/OydF4Z41U4zX/RWutlVJO2tx3x+7vKuAZpdTLWuu3cj3WBcQTwH9qrU8qpf6B6NvIxXM8pkLhRaL/Xk8opT4O7ALOnNshgVJqMfAocJPWemi658u7QK+1/lgOztEd+3lEKfUY0dfnGQn0ORhvN2CewdXHts0IqcarlPqLUqpGa90be1U84nAO4/4eUkr9kuisZLYCfSb3y9inSylVDFQAA7MzvCTSjldrbR7bA0TXSuYzs/pvdjqYg6jW+qdKqfuUUpVa6zkzO1NKuYkG+R9qrXfa7JL1/V1wqRulVLlS6hTjd6AJsF2RnyfsA85USp2ulCohung460qWGLuBz8R+/wyQ9EailFqqlFoU+70SWAf8ftZGmNn9Mn+PK4FndGyVaw5IO96E/OtlRPO285ndwKdj6pALgEFTym9eoZSqNtZnlFIfIBoT5+qhT2ws3wP+oLX+lsNu2d/fuV5lzvGK9RVE81Ungb8Ae2Lba4Gfxn5fRVTZ8DvgVaIplHk7Xj25yv5HorPiuRzvcuBp4A3g58Cy2Pa1wAOx3y8EXo7d35eBv5+DcSbdL+A24LLY76XAw8CbwG+BVXP87zbdeLfH/q3+DvgF8FdzPN7/BHqBUOzf798D/wj8Y+xzBfxL7Pu8TAoF3DwY6z+Z7u0LwIVzfG/XE11DPAi8FPvz8eneX7FAEARBKHAWXOpGEARhoSGBXhAEocCRQC8IglDgSKAXBEEocCTQC4IgFDgS6AVBEAocCfSCIAgFzv8PD1L5iJEZ7S8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if D1:\n",
    "    plt.scatter(x_train[:,0], y_train);\n",
    "    plt.scatter(x_validation[:,0], y_validation);\n",
    "    plt.scatter(x_test[:,0], y_test);\n",
    "else:\n",
    "    plt.scatter(x_train[:,1], y_train);\n",
    "    plt.scatter(x_validation[:,1], y_validation);\n",
    "    plt.scatter(x_test[:,1], y_test);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zac2HHNlgbpm"
   },
   "outputs": [],
   "source": [
    "# convert from nparray to Var\n",
    "def nparray_to_Var(x):\n",
    "    if x.ndim==1:\n",
    "        y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
    "    else:\n",
    "        y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
    "    return y\n",
    "   \n",
    "x_train = nparray_to_Var(x_train)\n",
    "y_train = nparray_to_Var(y_train)\n",
    "x_validation = nparray_to_Var(x_validation)\n",
    "y_validation = nparray_to_Var(y_validation)\n",
    "x_test = nparray_to_Var(x_test)\n",
    "y_test = nparray_to_Var(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbjrqcpVFtGe"
   },
   "source": [
    "# Defining and initializing the network\n",
    "\n",
    "The steps to create a feed forward neural network are the following:\n",
    "\n",
    "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. The number of features in X and the output dimensionality (the size of Y) are given but the numbers in between are set by the researcher. Remember that for each unit in each layer beside in the input has a bias term.\n",
    "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
    "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
    "\n",
    "In order to make life easier for ourselves we define a DenseLayer class that takes care of initialization and the forward pass. We can also extend it later with print and advanced initialization capabilities. For the latter we have introduced a Initializer class.\n",
    "\n",
    "Note that we use Sequence in the code below. A Sequence is an ordered list. This means the order we insert and access items are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ij_ieRsAt7Xt"
   },
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    def init_weights(self, n_in, n_out):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def init_bias(self, n_out):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eb18N5phuIha"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NormalInitializer(Initializer):\n",
    "\n",
    "    def __init__(self, mean=0, std=0.1):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def init_weights(self, n_in, n_out):\n",
    "        return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "    def init_bias(self, n_out):\n",
    "        return [Var(0.0) for _ in range(n_out)]\n",
    "\n",
    "class ConstantInitializer(Initializer):\n",
    "\n",
    "    def __init__(self, weight=1.0, bias=0.0):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "\n",
    "    def init_weights(self, n_in, n_out):\n",
    "        return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "    def init_bias(self, n_out):\n",
    "        return [Var(self.bias) for _ in range(n_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jOLYGnZKuM6W"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
    "        self.weights = initializer.init_weights(n_in, n_out)\n",
    "        self.bias = initializer.init_bias(n_out)\n",
    "        self.act_fn = act_fn\n",
    "    \n",
    "    def __repr__(self):    \n",
    "        return 'Weights: ' + repr(self.weights) + ' Biases: ' + repr(self.bias)\n",
    "\n",
    "    def parameters(self) -> Sequence[Var]:\n",
    "        params = []\n",
    "        for r in self.weights:\n",
    "            params += r\n",
    "\n",
    "        return params + self.bias\n",
    "\n",
    "    def forward(self, inputs: Sequence[Var]) -> Sequence[Var]:\n",
    "        assert len(self.weights) == len(inputs), \"weights and inputs must match in first dimension\"\n",
    "        weights = self.weights\n",
    "        out = []\n",
    "        for j in range(len(weights[0])):\n",
    "            node = self.bias[j] # <- Insert code\n",
    "            for i in range(len(inputs)):\n",
    "                node += inputs[i]*weights[0][j]  # <- Insert code\n",
    "            node = self.act_fn(node)\n",
    "            out.append(node)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_8n_SKnIW2F"
   },
   "source": [
    "## Exercise f) Complete the forward pass\n",
    "\n",
    "In the code below we initialize a 1-5-1 network and pass the training set through it. The forward method in DenseLayer is not complete. It  just outputs zeros right now. The method forward should perform an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) on the input followed by an application of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDEjtePxE7Mv",
    "outputId": "271bcd73-43d6-4255-8ea0-6447b2a0ad8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Var(v=-0.0112, grad=0.0000)], [Var(v=-0.0225, grad=0.0000)], [Var(v=-0.0020, grad=0.0000)], [Var(v=-0.0339, grad=0.0000)], [Var(v=-0.0166, grad=0.0000)], [Var(v=-0.0153, grad=0.0000)], [Var(v=-0.0099, grad=0.0000)], [Var(v=-0.0081, grad=0.0000)], [Var(v=-0.0349, grad=0.0000)], [Var(v=-0.0062, grad=0.0000)], [Var(v=-0.0173, grad=0.0000)], [Var(v=-0.0215, grad=0.0000)], [Var(v=-0.0139, grad=0.0000)], [Var(v=-0.0246, grad=0.0000)], [Var(v=-0.0074, grad=0.0000)], [Var(v=-0.0144, grad=0.0000)], [Var(v=-0.0266, grad=0.0000)], [Var(v=-0.0126, grad=0.0000)], [Var(v=-0.0013, grad=0.0000)], [Var(v=-0.0044, grad=0.0000)], [Var(v=-0.0161, grad=0.0000)], [Var(v=-0.0071, grad=0.0000)], [Var(v=-0.0194, grad=0.0000)], [Var(v=-0.0196, grad=0.0000)], [Var(v=-0.0094, grad=0.0000)], [Var(v=-0.0254, grad=0.0000)], [Var(v=-0.0252, grad=0.0000)], [Var(v=-0.0277, grad=0.0000)], [Var(v=-0.0029, grad=0.0000)], [Var(v=-0.0175, grad=0.0000)], [Var(v=-0.0318, grad=0.0000)], [Var(v=-0.0042, grad=0.0000)], [Var(v=-0.0277, grad=0.0000)], [Var(v=-0.0300, grad=0.0000)], [Var(v=-0.0133, grad=0.0000)], [Var(v=-0.0056, grad=0.0000)], [Var(v=-0.0344, grad=0.0000)], [Var(v=-0.0130, grad=0.0000)], [Var(v=-0.0244, grad=0.0000)], [Var(v=-0.0174, grad=0.0000)], [Var(v=-0.0123, grad=0.0000)], [Var(v=-0.0166, grad=0.0000)], [Var(v=-0.0261, grad=0.0000)], [Var(v=-0.0208, grad=0.0000)], [Var(v=-0.0164, grad=0.0000)], [Var(v=-0.0200, grad=0.0000)], [Var(v=-0.0167, grad=0.0000)], [Var(v=-0.0159, grad=0.0000)], [Var(v=-0.0076, grad=0.0000)], [Var(v=-0.0162, grad=0.0000)], [Var(v=-0.0071, grad=0.0000)], [Var(v=-0.0006, grad=0.0000)], [Var(v=-0.0176, grad=0.0000)], [Var(v=-0.0169, grad=0.0000)], [Var(v=-0.0280, grad=0.0000)], [Var(v=-0.0246, grad=0.0000)], [Var(v=-0.0112, grad=0.0000)], [Var(v=-0.0125, grad=0.0000)], [Var(v=-0.0092, grad=0.0000)], [Var(v=-0.0298, grad=0.0000)], [Var(v=-0.0164, grad=0.0000)], [Var(v=-0.0210, grad=0.0000)], [Var(v=-0.0347, grad=0.0000)], [Var(v=-0.0080, grad=0.0000)], [Var(v=-0.0058, grad=0.0000)], [Var(v=-0.0285, grad=0.0000)], [Var(v=-0.0305, grad=0.0000)], [Var(v=-0.0207, grad=0.0000)], [Var(v=-0.0218, grad=0.0000)], [Var(v=-0.0081, grad=0.0000)], [Var(v=-0.0084, grad=0.0000)], [Var(v=-0.0038, grad=0.0000)], [Var(v=-0.0160, grad=0.0000)], [Var(v=-0.0094, grad=0.0000)], [Var(v=-0.0030, grad=0.0000)], [Var(v=-0.0254, grad=0.0000)], [Var(v=-0.0151, grad=0.0000)], [Var(v=-0.0146, grad=0.0000)], [Var(v=-0.0192, grad=0.0000)], [Var(v=-0.0264, grad=0.0000)], [Var(v=-0.0040, grad=0.0000)], [Var(v=-0.0024, grad=0.0000)], [Var(v=-0.0313, grad=0.0000)], [Var(v=-0.0043, grad=0.0000)], [Var(v=-0.0159, grad=0.0000)], [Var(v=-0.0069, grad=0.0000)], [Var(v=-0.0058, grad=0.0000)], [Var(v=-0.0177, grad=0.0000)], [Var(v=-0.0323, grad=0.0000)], [Var(v=-0.0336, grad=0.0000)], [Var(v=-0.0096, grad=0.0000)], [Var(v=-0.0102, grad=0.0000)], [Var(v=-0.0187, grad=0.0000)], [Var(v=-0.0150, grad=0.0000)], [Var(v=-0.0267, grad=0.0000)], [Var(v=-0.0321, grad=0.0000)], [Var(v=-0.0069, grad=0.0000)], [Var(v=-0.0011, grad=0.0000)], [Var(v=-0.0239, grad=0.0000)], [Var(v=-0.0189, grad=0.0000)], [Var(v=-0.0157, grad=0.0000)], [Var(v=-0.0184, grad=0.0000)], [Var(v=-0.0282, grad=0.0000)], [Var(v=-0.0076, grad=0.0000)], [Var(v=-0.0223, grad=0.0000)]]\n"
     ]
    }
   ],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 5, lambda x: x.relu()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "def forward(input, network):\n",
    "\n",
    "    def forward_single(x, network):\n",
    "        for layer in network:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    output = [ forward_single(input[n], network) for n in range(len(input))]\n",
    "    return output\n",
    "\n",
    "print(forward(x_train, NN))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLrGJytZFtGm"
   },
   "source": [
    "## Exercise g) Print all network parameters\n",
    "\n",
    "Make a function that prints all the parameters of the network (weights and biases) with information about in which layer the appear. In the object oriented spirit you should introduce a method in the DenseLayer class to print the parameters of a layer. Hint: You can take inspiration from the corresponding method in Var. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iac-VwYGFtGm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: \n",
      "Weights: [[Var(v=-0.0506, grad=0.0000), Var(v=-0.1067, grad=0.0000), Var(v=0.0344, grad=0.0000), Var(v=0.1172, grad=0.0000), Var(v=-0.0603, grad=0.0000)]] \n",
      "Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)] \n",
      "\n",
      "Layer 2: \n",
      "Weights: [[Var(v=-0.1023, grad=0.0000)], [Var(v=-0.0820, grad=0.0000)], [Var(v=-0.1325, grad=0.0000)], [Var(v=0.0016, grad=0.0000)], [Var(v=-0.0694, grad=0.0000)]] \n",
      "Biases: [Var(v=0.0000, grad=0.0000)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The repr function is defined again (already defined above):\n",
    "\n",
    "def new_repr(self):\n",
    "    return f'Weights: {repr(self.weights)} \\nBiases: {repr(self.bias)}'\n",
    "\n",
    "DenseLayer.__repr__ = new_repr\n",
    "\n",
    "def show_layers(network):\n",
    "    for i, n in enumerate(network):\n",
    "        print(f'Layer {i+1}: \\n{n} \\n')\n",
    "        \n",
    "show_layers(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-u4xk_ORFtGz"
   },
   "source": [
    "# Activation functions\n",
    "\n",
    "To have a full definition of the neural network, we must define an activation function for every layer. Several activation functions have been proposed and have different characteristics. In the Var class we have already defined the rectified linear init (relu). \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpIZPBpNI0pO"
   },
   "source": [
    "## Exercise h) Add more activation functions\n",
    "\n",
    "Implement the following activation functions in the Var class:\n",
    "\n",
    "* Identity: $$\\mathrm{identity}(x) = x$$\n",
    "* Hyperbolic tangent: $$\\tanh(x)$$\n",
    "* Sigmoid (or logistic function): $$\\mathrm{sigmoid}(x) = \\frac{1}{1.0 + \\exp(-x ) }$$  Hint: $\\mathrm{sigmoid}'(x)= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x))$.  \n",
    "\n",
    "Hint: You can seek inspiration in the relu method in the Var class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_79HOAXrFtHK"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Now that we have defined our activation functions we can visualize them to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "1FcylHqLTl-Z",
    "outputId": "c23c865b-3894-434c-f38a-f3cff5f772e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21da9336310>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "# convert from Var to ndarray  \n",
    "def Var_to_nparray(x):\n",
    "    y = np.zeros((len(x),len(x[0])))\n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(x[0])):\n",
    "            y[i,j] = x[i][j].v\n",
    "    return y\n",
    "\n",
    "# define 1-1 network with weight = 1 and relu activation \n",
    "NN = [ DenseLayer(1, 1, lambda x: x.relu(), initializer = ConstantInitializer(1.0)) ] \n",
    "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
    "\n",
    "#y = Var_to_nparray(relu(nparray_to_Var(x)))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "oOL2UolJFtHL",
    "outputId": "9d51d191-ca43-44a6-d5f6-d41a2bd0a80b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing all activation layers\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "units = {\n",
    "    \"identity\": lambda x: x.identity(),\n",
    "    \"sigmoid\": lambda x: x.sigmoid(),\n",
    "    \"relu\": lambda x: x.relu(),\n",
    "    \"tanh\": lambda x: x.tanh()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "[plt.plot(x, Var_to_nparray(forward(nparray_to_Var(x), [DenseLayer(1, 1, unit, initializer = ConstantInitializer(1.0))]) ), label=unit_name, lw=2) for unit_name, unit in units.items()] # unit(nparray_to_Var(x))), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Our activation functions', fontsize=20)\n",
    "plt.ylim([-2, 5])\n",
    "plt.xlim([-6, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-jdEl-7FtGs"
   },
   "source": [
    "# Advanced initialization schemes\n",
    "\n",
    "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
    "\n",
    "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
    "\n",
    "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
    "\n",
    "The Glorot initialization has the form: \n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
    "\n",
    "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
    "\n",
    "The He initialization is very similar\n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqeyab9qFtGs"
   },
   "source": [
    "## Exercise i) Glorot and He initialization\n",
    " \n",
    "Using the Initializer class, implement functions that implement Glorot and He \n",
    "\n",
    "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper.\n",
    "\n",
    "Comment: If you want to be more advanced then try to make a universal initializer taking both the activation function and type (Glorot or He) as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Qyk01CgaFtGt"
   },
   "outputs": [],
   "source": [
    "## Glorot\n",
    "def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
    "    alpha = 1 # for tanh\n",
    "    std = np.sqrt(2*alpha/(n_in+n_out)) # <- replace with proper initialization\n",
    "    return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
    "\n",
    "## He\n",
    "def DenseLayer_He_relu(n_in: int, n_out: int):\n",
    "    alpha = 2\n",
    "    std = np.sqrt(alpha/n_in) # <- replace with proper initialization\n",
    "    return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:\n",
    "\n",
    "> To test if the initializatios have the desired property, it's possible to look at weights of the layers after each mini-batch, after it has done a foward and backward pass. Then, it's possible to find the standard deviations and the mean of the weights. These can then be plotted to see if they haven't changed too much from the distribution used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XyXBD37FtHk"
   },
   "source": [
    "## Exercise j) Forward pass unit test\n",
    "\n",
    "Write a bit of code to make a unit test that the forward pass works. This can be done by defining a simple network with for example all weights equal to one (using the ConstantInitializer method) and identity activation functions. \n",
    "\n",
    "Hints: Use the [assert](https://www.w3schools.com/python/ref_keyword_assert.asp), the nparray_to_Var and the Var_to_nparray commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "k0miqRUAFtHl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37728416]\n",
      " [0.0200712 ]\n",
      " [0.32207917]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "simple_N = [DenseLayer(1, 1, lambda x: x.identity(), initializer=ConstantInitializer(1.0))]\n",
    "\n",
    "inp = np.random.rand(3, 1)\n",
    "\n",
    "def simple_test(inp, network):\n",
    "    output = Var_to_nparray(forward(nparray_to_Var(inp), network))\n",
    "    assert np.array_equal(inp, output)  # Checks if arrays are equal, for when more than one sample\n",
    "    print(output)\n",
    "\n",
    "simple_test(inp, simple_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faCxhfFnFtHp"
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "We are only missing a loss function to we need to define a loss function and its derivative with respect to the output of the neural network $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "I2eDYKvAFtHq"
   },
   "outputs": [],
   "source": [
    "def squared_loss(t, y):\n",
    "  \n",
    "    # add check that sizes agree\n",
    "  \n",
    "    def squared_loss_single(t, y):\n",
    "        Loss = Var(0.0)\n",
    "        for i in range(len(t)): # sum over outputs\n",
    "            Loss += (t[i]-y[i]) ** 2\n",
    "        return Loss\n",
    "\n",
    "    Loss = Var(0.0)\n",
    "    for n in range(len(t)): # sum over training data\n",
    "        Loss += squared_loss_single(t[n],y[n])\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrwSJ2UWFtHu"
   },
   "source": [
    "## Exercise j) Implement cross entropy loss\n",
    "\n",
    "Insert code below to implement cross-entropy loss for general dimensionality of $t$. Use a logits formulation:\n",
    "$$\n",
    "\\rm{Loss} = - \\sum_i t_i \\, log \\, p_i \n",
    "$$\n",
    "with $p$ given by the the softmax function in terms of the logits $h$:\n",
    "$$\n",
    "p_i = \\frac{\\exp(h_i)}{\\sum_{i'} \\exp(h_{i'} } .\n",
    "$$\n",
    "Inserting $p$ in the expression for the loss gives\n",
    "$$\n",
    "\\rm{Loss} = - \\sum_i t_i h_i + \\rm{LogSumExp}(h) \\ ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\rm{LogSumExp}(h) = \\log \\sum_i \\exp h_i \\ .\n",
    "$$\n",
    "This is true for $t$ being a one-hot vector. In practice you want to implement a [numerically stable](https://leimao.github.io/blog/LogSumExp/) version of LogSumExp. But we will not bother about that here.\n",
    "\n",
    "Help: You can add these methods in the Var class:\n",
    "\n",
    "    def exp(self):\n",
    "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
    "    \n",
    "    def log(self):\n",
    "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6nMuxyfzFtHv"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(t, y):\n",
    "     \n",
    "    Loss = Var(0.0)\n",
    "\n",
    "    logsumexp = lambda x: np.log(np.sum(np.exp(x)))\n",
    "    for i in range(len(y)):\n",
    "        Loss += logsumexp(y[i]) - np.sum(t[i]*y[i])\n",
    "    \n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    Loss = Var(0.0)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(t, y):    \n",
    "        def ce_loss(t, y):\n",
    "        a = Var(0.0)\n",
    "        b = Var(0.0)\n",
    "        for y_i in y:\n",
    "            a -= t[i]*y[i]\n",
    "            b += y_i.exp()\n",
    "        \n",
    "        return a + b.log()\n",
    "    \n",
    "    Loss = Var(0.0)\n",
    "    for i in range(len(y)):\n",
    "        Loss += ce_loss(t[i], y[i])\n",
    "    \n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fAF5ew4FtHy"
   },
   "source": [
    "# Backward pass\n",
    "\n",
    "Now the magic happens! We get the calculation of the gradients for free. Just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHyfPPI9Qqwu"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 5, lambda x: x.relu()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "output = forward(x_train, NN)\n",
    "\n",
    "Loss = squared_loss(y_train, output)\n",
    "Loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49biIAYKQ1oG"
   },
   "source": [
    "and the gradients will be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rGt1bq_Q7uk",
    "outputId": "8d283577-9bd1-496b-f4a8-764a9507a4bf"
   },
   "outputs": [],
   "source": [
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7d7qK0uFtH9"
   },
   "source": [
    "# Backward pass unit test\n",
    "\n",
    "Above we used finite differences to test that Nanograd is actually doing what it is supposed to do. We can in principle try the same for the neural network. But we will trust that the test above is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgBi8GOSFtIN"
   },
   "source": [
    "# Training and validation\n",
    "\n",
    "We are ready to train some neural networks!\n",
    "\n",
    "We initialize again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01ePmzBzRtdh"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 5, lambda x: x.relu()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "output = forward(x_train, NN)\n",
    "\n",
    "Loss = squared_loss(y_train,output)\n",
    "Loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10iRPiQ1ISHw"
   },
   "source": [
    "and make an update:\n",
    "\n",
    "We introduce a help function parameters to have a handle in all parameters in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhAI7eyeznia",
    "outputId": "8584bbad-00aa-4130-9961-44e9ae7fb4ed"
   },
   "outputs": [],
   "source": [
    "print('Network before update:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
    "\n",
    "def parameters(network):\n",
    "    params = []\n",
    "    for layer in range(len(network)):\n",
    "        params += network[layer].parameters()\n",
    "    return params\n",
    "\n",
    "def update_parameters(params, learning_rate=0.01):\n",
    "    for p in params:\n",
    "        p.v -= learning_rate*p.grad\n",
    "\n",
    "def zero_gradients(params):\n",
    "    for p in params:\n",
    "        p.grad = 0.0\n",
    "\n",
    "update_parameters(parameters(NN))\n",
    "\n",
    "print('\\nNetwork after update:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
    "\n",
    "zero_gradients(parameters(NN))\n",
    "\n",
    "print('\\nNetwork after zeroing gradients:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woWYpdw6FtIO"
   },
   "outputs": [],
   "source": [
    "# Initialize an arbitrary neural network\n",
    "NN = [\n",
    "    DenseLayer(1, 8, lambda x: x.relu()),\n",
    "    DenseLayer(8, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "# Recommended hyper-parameters for 3-D: \n",
    "#NN = [\n",
    "#    DenseLayer(3, 16, lambda x: x.relu()),\n",
    "#    DenseLayer(16, 1, lambda x: x.identity())\n",
    "#]\n",
    "\n",
    "\n",
    "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
    "## of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdqaqYBVFtIR"
   },
   "outputs": [],
   "source": [
    "# Initialize training hyperparameters\n",
    "EPOCHS = 200\n",
    "LEARN_R = 2e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kfg76GMFtIW",
    "outputId": "e30cf68a-31f2-42b4-cc5e-860c297c0f04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "     \n",
    "    # Forward pass and loss computation\n",
    "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
    "\n",
    "    # Backward pass\n",
    "    Loss.backward()\n",
    "    \n",
    "    # gradient descent update\n",
    "    update_parameters(parameters(NN), LEARN_R)\n",
    "    zero_gradients(parameters(NN))\n",
    "    \n",
    "    # Training loss\n",
    "    train_loss.append(Loss.v)\n",
    "    \n",
    "    # Validation\n",
    "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
    "    val_loss.append(Loss_validation.v)\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "VetyRWFwFtIY",
    "outputId": "344e490d-6d7d-455a-fa6f-88dd11eb957e"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss)), train_loss);\n",
    "plt.plot(range(len(val_loss)), val_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OgmIrM9FtIb"
   },
   "source": [
    "# Testing\n",
    "\n",
    "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmNi7S-vFtIc"
   },
   "outputs": [],
   "source": [
    "output_test = forward(x_test, NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "7mmJOTSEFtIf",
    "outputId": "e3264095-cefe-4aee-893d-bf152438e332"
   },
   "outputs": [],
   "source": [
    "y_test_np = Var_to_nparray(y_test)\n",
    "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
    "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
    "plt.xlabel(\"y\");\n",
    "plt.ylabel(\"$\\hat{y}$\");\n",
    "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "plt.grid(True);\n",
    "plt.axis('equal');\n",
    "plt.tight_layout();\n",
    "\n",
    "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
    "\n",
    "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ODi0WlmQFtIh",
    "outputId": "d1ab874f-0717-4987-87bf-1f0c7c8e7148"
   },
   "outputs": [],
   "source": [
    "x_test_np = Var_to_nparray(x_test)\n",
    "x_train_np = Var_to_nparray(x_train)\n",
    "y_train_np = Var_to_nparray(y_train)\n",
    "if D1:\n",
    "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
    "    plt.scatter(x_test_np, Var_to_nparray(output_test), label=\"test prediction\");\n",
    "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");\n",
    "else:\n",
    "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
    "    plt.scatter(x_test_np[:,1], Var_to_nparray(output_test), label=\"test data prediction\");\n",
    "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTBAmjsAFtIk"
   },
   "source": [
    "## Exercise k) Show overfitting, underfitting and just right fitting\n",
    "\n",
    "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
    "\n",
    "See also if you can get a good compromise which leads to a low validation loss. \n",
    "\n",
    "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
    "\n",
    "_Insert written answer here._\n",
    "\n",
    "### Answer:\n",
    "\n",
    "> Below we see that the test loss is:\n",
    "> - Lower than both validation and training loss, for the overfitting example\n",
    "> - Is very close to the validation loss for both the underfitting and just right example\n",
    "\n",
    "> The validation set is used to give an evaluation of the models fit on the training data, and tune the hyperparameters. So, it's important to keep the validation and test set seperate, so you have the possibility to evaluate the model on fully \"unseen\" data and give an unbiased estimate of the generalization error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a function that creates a network, trains it and plot the train/validation loss, to minimize the amount of repeated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_network(n_hidden: int=5, epochs: int=200, lr: float=2e-3):\n",
    "    NN = [\n",
    "        DenseLayer(1, n_hidden, lambda x: x.relu()),\n",
    "        DenseLayer(n_hidden, 1, lambda x: x.identity())\n",
    "    ]\n",
    "\n",
    "    # Initialize training hyperparameters\n",
    "    EPOCHS = epochs\n",
    "    LEARN_R = lr\n",
    "\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "\n",
    "        # Forward pass and loss computation\n",
    "        Loss = squared_loss(y_train, forward(x_train, NN))\n",
    "\n",
    "        # Backward pass\n",
    "        Loss.backward()\n",
    "\n",
    "        # gradient descent update\n",
    "        update_parameters(parameters(NN), LEARN_R)\n",
    "        zero_gradients(parameters(NN))\n",
    "\n",
    "        # Training loss\n",
    "        train_loss.append(Loss.v)\n",
    "\n",
    "        # Validation\n",
    "        Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
    "        val_loss.append(Loss_validation.v)\n",
    "\n",
    "        if e%10==0:\n",
    "            print(\"{:4d}\".format(e),\n",
    "                  \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "                  \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "\n",
    "    plt.plot(range(len(train_loss)), train_loss, label='train')\n",
    "    plt.plot(range(len(val_loss)), val_loss, label='val')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    # Test set\n",
    "    output_test = forward(x_test, NN)\n",
    "\n",
    "    y_test_np = Var_to_nparray(y_test)\n",
    "    plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
    "    plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
    "    plt.xlabel(\"y\");\n",
    "    plt.ylabel(\"$\\hat{y}$\");\n",
    "    plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "    plt.grid(True);\n",
    "    plt.axis('equal');\n",
    "    plt.tight_layout();\n",
    "\n",
    "    Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
    "\n",
    "    print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "For overfitting we're using an architecture with 4 hidden units. The learning rate and amount of epochs stays the same as above. We see that the model overfit the training data as it has a much lower training loss than validation loss. We also see that the model also is past it's optimal performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_network(n_hidden=4, epochs=200, lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "\n",
    "Here we use the same architechture, but use a very low learning rate of $1^{-4}$ and 150 epochs. We see that the model barely reaches a minimum of loss and will therefore not perform optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQZCn2dxFtIl"
   },
   "outputs": [],
   "source": [
    "test_network(n_hidden=4, epochs=150, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just right\n",
    "\n",
    "Here we use an architecture with 5 hidden units, while for training we use 200 epochs and a learning rate of $1^{-3}$. This results in a model that seems to have reached a good \"plateau\" for the loss value. Here, the validation loss is even lower than the training loss. Though this model probably could get event better with a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_network(n_hidden=5, epochs=200, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYPZP-eTFtIo"
   },
   "source": [
    "# Next steps - classification\n",
    "\n",
    "It is straight forward to extend what we have done to classification. \n",
    "\n",
    "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
    "\n",
    "Next week we will see how to perform classification in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsVPul3QFtIo"
   },
   "source": [
    "## Exercise l) optional - Implement backpropagation for classification\n",
    "\n",
    "Should be possible with very few lines of code. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC8QrI2tFtIp"
   },
   "outputs": [],
   "source": [
    "# Just add code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APqhJv3tta1O"
   },
   "source": [
    "## Exercise m) optional - Introduce a NeuralNetwork class\n",
    "\n",
    "The functions we applied on the neural network (parameters, update_parameters and zero_gradients) can more naturally be included as methods in a NeuralNetwork class. Make such a class and modify the code to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dqfnor1ouMLq"
   },
   "outputs": [],
   "source": [
    "# just add some code"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "2.1-EXE-FNN-AutoDif-Nanograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
